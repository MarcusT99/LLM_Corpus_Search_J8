{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d2ec61",
   "metadata": {},
   "source": [
    "## This notebook is an end to end build of your initial database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220ae2",
   "metadata": {},
   "source": [
    "Load your packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from datetime import datetime, date\n",
    "import re\n",
    "import time\n",
    "from enum import Enum\n",
    "\n",
    "# External Libraries\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, PydanticDeprecatedSince20, Field\n",
    "from huggingface_hub.utils import _deprecation\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Langchain specific\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# LanceDB specific\n",
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf03667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load functions\n",
    "\n",
    "def Salty_Detective(root_path: str, extensions: List[str], export_csv: bool = False) -> List[Tuple[str, str, int]]:\n",
    "    \"\"\"\n",
    "    Scans through all subfolders from root_path and identifies files\n",
    "    matching user-defined extensions. Optionally exports to CSV.\n",
    "\n",
    "    Parameters:\n",
    "        root_path (str): The directory path to search.\n",
    "        extensions (List[str]): File extensions to search for (e.g., ['.pdf', '.txt']).\n",
    "        export_csv (bool): If True, saves results to a timestamped CSV and opens it.\n",
    "\n",
    "    Returns:\n",
    "        List of (filename, full_path, is_repeated)\n",
    "    \"\"\"\n",
    "    extensions = [ext.lower() for ext in extensions]\n",
    "    collected_files = []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        for file in filenames:\n",
    "            if any(file.lower().endswith(ext) for ext in extensions):\n",
    "                full_path = os.path.join(dirpath, file)\n",
    "                collected_files.append((file, full_path))\n",
    "\n",
    "    name_counts = Counter([name for name, _ in collected_files])\n",
    "    results = [(name, path, int(name_counts[name] > 1)) for name, path in collected_files]\n",
    "    df = pd.DataFrame(results, columns=[\"File Name\", \"Path\", \"Does it Repeat (0 or 1)\"])\n",
    "    if export_csv:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"Salty_Detective_Results_{timestamp}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\n CSV exported as: {filename}\")\n",
    "\n",
    "        try:\n",
    "            os.startfile(filename)\n",
    "        except Exception as e:\n",
    "            print(f\"There is an issue opening the file automatically: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_metadata(text: str, file_path: str) -> dict:\n",
    "    '''\n",
    "    Function to scrape keywords from the chunks\n",
    "    '''\n",
    "    prompt = f\"\"\"\n",
    "Given the following text, extract:\n",
    "\n",
    "1. A list of 10–20 keywords (comma-separated)\n",
    "\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Return in this format:\n",
    "KEYWORDS: ...\n",
    "\"\"\"\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    lines = response.content.splitlines()\n",
    "\n",
    "    keywords = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"KEYWORDS:\"):\n",
    "            keywords = [kw.strip() for kw in line[len(\"KEYWORDS:\"):].split(\",\")]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"key_words\": keywords,\n",
    "        \"file_extension\": file_path  # full path used as metadata\n",
    "    }\n",
    "\n",
    "def safe_embed_documents(rows, max_chars=3000):\n",
    "    \"\"\"\n",
    "    Tries to embed all rows. If a 413 error occurs, truncates long rows and retries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        texts = [row[\"text\"] for row in rows]\n",
    "        return tei_endpoint.embed_documents(texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"413\" in str(e) or \"Payload Too Large\" in str(e):\n",
    "            print(\"Payload too large. Truncating and retrying...\")\n",
    "\n",
    "            truncated_rows = []\n",
    "            for row in rows:\n",
    "                text = row[\"text\"]\n",
    "                if len(text) > max_chars:\n",
    "                    row = row.copy()\n",
    "                    row[\"text\"] = text[:max_chars]\n",
    "                truncated_rows.append(row)\n",
    "\n",
    "            texts = [row[\"text\"] for row in truncated_rows]\n",
    "            return tei_endpoint.embed_documents(texts)\n",
    "\n",
    "        else:\n",
    "            raise  # re-raise unknown errors\n",
    "\n",
    "def is_bad_ocr(text, symbol_threshold=0.3, short_line_ratio=0.5, gibberish_line_ratio=0.5):\n",
    "    '''\n",
    "    function to search for poor lines of text in a md format, adjustable threshold if OCR has a known type of failure to look for\n",
    "    '''\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    total_lines = len(lines)\n",
    "    if total_lines == 0:\n",
    "        return True\n",
    "\n",
    "    # Count lines with mostly non-alphanumerics\n",
    "    noisy_lines = sum(1 for line in lines if len(re.findall(r'[^\\w\\s]', line)) / (len(line) + 1e-5) > 0.5)\n",
    "\n",
    "    # Count short lines\n",
    "    short_lines = sum(1 for line in lines if len(line.strip()) <= 3)\n",
    "\n",
    "    # Detect gibberish lines (e.g., repeated single characters)\n",
    "    gibberish_lines = sum(1 for line in lines if re.match(r'^([^\\w\\s])\\1{2,}$', line.strip()))\n",
    "\n",
    "    # Check overall symbol frequency\n",
    "    total_chars = len(text)\n",
    "    symbol_count = len(re.findall(r'[^\\w\\s]', text))\n",
    "    symbol_ratio = symbol_count / (total_chars + 1e-5)\n",
    "\n",
    "    # Calculate line ratios\n",
    "    short_line_ratio_actual = short_lines / total_lines\n",
    "    gibberish_ratio_actual = gibberish_lines / total_lines\n",
    "\n",
    "    # Determine bad OCR based on thresholds\n",
    "    if (\n",
    "        symbol_ratio > symbol_threshold or\n",
    "        short_line_ratio_actual > short_line_ratio or\n",
    "        gibberish_ratio_actual > gibberish_line_ratio\n",
    "    ):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def clean_ocr_text(text):\n",
    "    \n",
    "    '''\n",
    "    A function to clean poor OCR of excess symbols, white spacea and repeated character that don't make sense\n",
    "    '''\n",
    "    # Remove ASCII noise lines (e.g., lines full of ~, -, _, etc.)\n",
    "    text = re.sub(r'^[~\\-_\\\\\\/\\.\\'\\*\\s]{3,}$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    # Normalize newlines (remove multiple blank lines)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "\n",
    "    # Remove control characters and non-printable ASCII\n",
    "    text = re.sub(r'[^\\x20-\\x7E\\n]', '', text)\n",
    "\n",
    "    # Optional: remove leading/trailing whitespace from lines\n",
    "    text = \"\\n\".join([line.strip() for line in text.splitlines()])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec539e1",
   "metadata": {},
   "source": [
    "### Step 1: Get all your file extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787862f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Path</th>\n",
       "      <th>Does it Repeat (0 or 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PORTABLE_CASTLE_Quick_Look_Final_20180601.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRESCIENT VISION Quick Look FINAL.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROSAIC TWAIN_Quick Look_V4.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VISTA QUAKE Quick Look Final.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(U)_ACV11_Final_Report.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Wargaming_Effectiveness_2006.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Warriors_Edge.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>WG_6-13_CTOC_Report.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Wing_and_a_Prayer.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>wmd_Wargaming_1994.pdf</td>\n",
       "      <td>/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         File Name  \\\n",
       "0    PORTABLE_CASTLE_Quick_Look_Final_20180601.pdf   \n",
       "1            PRESCIENT VISION Quick Look FINAL.pdf   \n",
       "2                  PROSAIC TWAIN_Quick Look_V4.pdf   \n",
       "3                 VISTA QUAKE Quick Look Final.pdf   \n",
       "4                       (U)_ACV11_Final_Report.pdf   \n",
       "..                                             ...   \n",
       "163               Wargaming_Effectiveness_2006.pdf   \n",
       "164                              Warriors_Edge.pdf   \n",
       "165                        WG_6-13_CTOC_Report.pdf   \n",
       "166                          Wing_and_a_Prayer.pdf   \n",
       "167                         wmd_Wargaming_1994.pdf   \n",
       "\n",
       "                                                  Path  \\\n",
       "0    /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "1    /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "2    /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "3    /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "4    /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "..                                                 ...   \n",
       "163  /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "164  /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "165  /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "166  /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "167  /mnt/c/Users/kowal/OneDrive/Documents/NPS/Quar...   \n",
       "\n",
       "     Does it Repeat (0 or 1)  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "..                       ...  \n",
       "163                        0  \n",
       "164                        0  \n",
       "165                        0  \n",
       "166                        0  \n",
       "167                        0  \n",
       "\n",
       "[168 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Example Setup ===\n",
    "#/home/dylan/oa4910_folder/\n",
    "folder = r\"/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quarter_7/OA4820_Capstone_Course/\" #\n",
    "#extensions_to_find = ['.pdf', '.txt', '.pptx', '.docx', '.xlsx', '.csv']  # all the formats we typically use\n",
    "extensions_to_find = ['.pdf']  # test with three different types\n",
    "\n",
    "# === Run Salty Detective ===\n",
    "results = Salty_Detective(folder, extensions_to_find, export_csv=False)\n",
    "\n",
    "# === Print Results ===\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00c4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths=results.Path.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12508c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/kowal/OneDrive/Documents/NPS/Quarter_7/OA4820_Capstone_Course/CapStone_PDFs_MAJ_Fritzschreck/CapStone_PDFs_MAJ_Fritzschreck/PORTABLE_CASTLE_Quick_Look_Final_20180601.pdf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b18f7",
   "metadata": {},
   "source": [
    "### Step 2: OCR doucments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28bf42",
   "metadata": {},
   "source": [
    "All OCR was done via linux on HPC (Hamming) at NPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=docling_batch\n",
    "#SBATCH --output=logs/docling_%A_%a.out\n",
    "#SBATCH --error=logs/docling_%A_%a.err\n",
    "#SBATCH --array=0-143        # Adjust this range to match number of PDFs\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --time=23:00:00\n",
    "#SBATCH --mem=16G\n",
    " \n",
    "## Activate your conda environment\n",
    "source /smallwork/$USER/comp3/bin/activate\n",
    "## conda activate DL\n",
    " \n",
    "## Get the filename for this task\n",
    "PDF=$(sed -n \"$((SLURM_ARRAY_TASK_ID+1))p\" pdf_list.txt)\n",
    " \n",
    " \n",
    "## Run docling\n",
    "docling --from pdf --to md --vlm-model smoldocling --image-export-mode referenced --output ./docling_output/ --verbose \"$PDF\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ddf1a",
   "metadata": {},
   "source": [
    "###  Step 3: Establish connection to your embedding model and generating LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35031eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Genrating LLM: casperhansen/llama-3.3-70b-instruct-awq\n",
      "Model max context window: 131072\n",
      "Connected to small LLM: Qwen/Qwen2.5-7B-Instruct\n",
      "Model max context window: 131072\n",
      "Connected to embedding LLM: nomic-ai/nomic-embed-text-v1.5\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the model\n",
    "endpoint_path = \"gpu3\"\n",
    "info = os.popen(f\"curl -s http://trac-malenia.ern.nps.edu:8080/{endpoint_path}/info\").read()\n",
    "repo_id = \"/\".join(json.loads(info)['model_id'].split('/')[-2:])\n",
    "\n",
    "print(f\"Connected to Genrating LLM: {repo_id}\")\n",
    "\n",
    "# Create the wrapper for the endpoint\n",
    "tgi_endpoint = HuggingFaceEndpoint(\n",
    "    endpoint_url=f\"http://trac-malenia.ern.nps.edu:8080/{endpoint_path}\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.0,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Patch to avoid HuggingFace deprecation warnings\n",
    "class LocalChatHuggingFace(ChatHuggingFace):\n",
    "    def _resolve_model_id(self):\n",
    "        pass\n",
    "\n",
    "llm = LocalChatHuggingFace(\n",
    "    llm=tgi_endpoint,\n",
    "    model_id=repo_id,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# check model's max context window\n",
    "print(f'Model max context window: {llm.tokenizer.model_max_length}')\n",
    "\n",
    "\n",
    "# Define the path to the small model\n",
    "endpoint_path2 = \"gpu1\"\n",
    "info = os.popen(f\"curl -s http://trac-malenia.ern.nps.edu:8080/{endpoint_path2}/info\").read()\n",
    "repo_id2 = \"/\".join(json.loads(info)['model_id'].split('/')[-2:])\n",
    "\n",
    "print(f\"Connected to small LLM: {repo_id2}\")\n",
    "\n",
    "# Create the wrapper for the endpoint\n",
    "tgi_endpoint2 = HuggingFaceEndpoint(\n",
    "    endpoint_url=f\"http://trac-malenia.ern.nps.edu:8080/{endpoint_path2}\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.0,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Patch to avoid HuggingFace deprecation warnings\n",
    "class LocalChatHuggingFace(ChatHuggingFace):\n",
    "    def _resolve_model_id(self):\n",
    "        pass\n",
    "\n",
    "llm2 = LocalChatHuggingFace(\n",
    "    llm=tgi_endpoint2,\n",
    "    model_id=repo_id2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# check model's max context window\n",
    "print(f'Model max context window: {llm2.tokenizer.model_max_length}')\n",
    "\n",
    "\n",
    "endpoint_path3 = \"gpu4\"\n",
    "info = os.popen(f\"curl -s http://trac-malenia.ern.nps.edu:8080/{endpoint_path3}/info\").read()\n",
    "repo_id3 = \"/\".join(json.loads(info)['model_id'].split('/')[-2:])\n",
    "\n",
    "print(f\"Connected to embedding LLM: {repo_id3}\")\n",
    "# Use same endpoint for embeddings\n",
    "tei_endpoint = HuggingFaceEndpointEmbeddings(\n",
    "    model=f\"http://trac-malenia.ern.nps.edu:8080/{endpoint_path3}/embed\"  # Adjust path as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a217891",
   "metadata": {},
   "source": [
    "### Step 4 Create Database Schema and establish metadata fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfbd038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkMetadata(BaseModel):\n",
    "    file_extension: Optional[str] = \"\"\n",
    "    title: Optional[str] = \"\"\n",
    "    date_of_pub: Optional[date] = None\n",
    "    domain: Optional[List[str]] = []\n",
    "    agency: Optional[List[str]] = []\n",
    "    cocom: Optional[List[str]] = []\n",
    "    country: Optional[List[str]] = []\n",
    "    category: Optional[str] = \"\"\n",
    "    purpose: Optional[str] = \"\"\n",
    "    key_words: Optional[List[str]] = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TextChunkSchema(LanceModel):\n",
    "    id: str\n",
    "    vector: Vector(768)\n",
    "    metadata: ChunkMetadata = ChunkMetadata()\n",
    "    text: str\n",
    "\n",
    "# initialize a local LanceDB database\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "\n",
    "# initialize an empty table in the database\n",
    "table = db.create_table(\"Capstone_test_table\",\n",
    "                        schema=TextChunkSchema,\n",
    "                        mode='overwrite', ### Only do overwrite if you want to wipe the table clean\n",
    "                        #partitioned_by=[\"year_of_exercise\", \"agency_supporting\", \"combatant_command\", \"countries\", \"key_words\"],\n",
    "                        #mode='append',  # Append to the table if it already exists\n",
    "                        #partitioned_by=[\"year_of_exercise\", \"agency_supporting\", \"combatant_command\", \"countries\", \"key_words\"],\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a366a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Domain(str, Enum):\n",
    "    Land = \"Land\"\n",
    "    Maritime = \"Maritime\"\n",
    "    Air = \"Air\"\n",
    "    Space =\"Space\"\n",
    "    Cyberspace= \"Cyberspace\"\n",
    "    Other= \"Other\" #create an out for the model if no domain match well!\n",
    "class COCOM(str, Enum):\n",
    "    INDOPACOM = \"INDOPACOM\"\n",
    "    PACOM = \"PACOM\"\n",
    "    CENTCOM = \"CENTCOM\"\n",
    "    EUCOM= \"EUCOM\"\n",
    "    AFRICOM = \"AFRICOM\"\n",
    "    NORTHCOM =\"NORTHCOM\"\n",
    "    SOUTHCOM = \"SOUTHCOM\"\n",
    "    SPACECOM =  \"SPACECOM\"\n",
    "    CYBERCOM = \"CYBERCOM\"\n",
    "    SOCOM = \"SOCOM\"\n",
    "    STRATCOM = \"STRATCOM\"\n",
    "    TRANSCOM = \"TRANSCOM\"\n",
    "    Other= \"Other\" #create an out for the model if no domain match well!\n",
    "class Wargame_type(str, Enum):\n",
    "    policy_decision = \"policy\"\n",
    "    capbility_requirement= 'capability'\n",
    "    conop_strategy = 'conop'\n",
    "    process_org = \"process\"\n",
    "    other= \"other\"\n",
    "class SearchSchema(BaseModel, use_enum_values=True):\n",
    "    title: str = Field(description=\"the title of the wargame paper\")\n",
    "    date_of_pub: date = Field(description=\"the date the paper was written, if only a year is found, use January 1st. If only a month and year is found, use the 1st of the month.\",\n",
    "                             example=[\"1999-12-01\",\"2001-01-30\",\"2010-04-15\"])\n",
    "    agency: List[str] = Field(description=\"the list of agencies that either sponsored or hosted the wargame\")\n",
    "    domain: List[Domain] = Field(description=\"the list of domains covered in the wargame, these are limited to Land, Maritime, Air, Space and Cyberspace. An other category is left for any wargame that does\" \\\n",
    "    \"not fit one of the previous listed domains.\" \\\n",
    "    \" Land: This domain encompasses the surface of the Earth, excluding the high-water mark, and includes land-based operations.\" \\\n",
    "    \" Maritime: This domain includes oceans, seas, bays, estuaries, islands, and coastal areas, along with the airspace above these areas.\" \\\n",
    "    \" Air: This domain refers to the atmosphere, extending from the Earth's surface to the altitude where its effects on operations become negligible.\" \\\n",
    "    \" Space: This domain includes the area above the atmosphere where the effects of atmospheric drag on airborne objects become negligible.\" \\\n",
    "    \" Cyberspace: This domain encompasses the interdependent networks of information technology infrastructures, the Internet, and other data, including computer systems and embedded processors.\" \\\n",
    "    \" Other: This domain is only to be selected if all other domains do not match well to the wargame.\")\n",
    "    country: List[str] = Field(description=\"This is a list of participating countries or countries that are part of the wargame\")\n",
    "    cocom: List[COCOM] = Field (description=\"the list of combatant commands associated with the wargame,\" \\\n",
    "    \" these are limited to AFRICOM, CENTCOM, CYBERCOM, EUCOM, INDOPACOM, NORTHCOM, SPACECOM, SOUTHCOM, STRATCOM, and TRANSCOM. An other category is left for any wargame that does\" \\\n",
    "    \"not fit one of the previous listed combatant commands.\" \\\n",
    "    \"AFRICOM includes all countries in Africa accept Egypt.\" \\\n",
    "    \"CENTCOM is includes Egypt, Yemen, Oman, United Arab Emirates, Qatar, Bahrain, Saudi Arabia, \" \\\n",
    "    \"Jordan, Israel, Lebanon, Syria, Iraq, Kuwait, Iran, Afganistan, Pakistan, Turkmenistan, Tajikistan, Uzbekistan, Kyrgyzstan and Kazakhstan.\" \\\n",
    "    \"CYBRECOM defends the United States, countering hostile cyber actors alongside our interagency, industry, and international partners. Thic COCOM is focused on cyberspace.\" \\\n",
    "    \"EUCOM is responsible for military operations in Europe and parts of Eurasia. EUCOM includes the following countries: Albania, Andorra, Austria, Belgium, Bosnia and Herzegovina, Bulgaria\" \\\n",
    "    \"Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Iceland, Ireland, Italy, Kosovo, Latvia, Liechtenstein, \" \\\n",
    "    \"Lithuania, Luxembourg, Malta, Moldova, Monaco, Montenegro, Netherlands, North Macedonia, Norway, Poland, Portugal, Romania, San Marino, Serbia, Slovakia, Slovenia, \" \\\n",
    "    \"Spain, Sweden, Switzerland, Ukraine, United Kingdom, Vatican City. EUCOM also includes the Artic\" \\\n",
    "    \"INDOPACOM includes 38 countries: China, Japan, Taiwan, South Korea (Republic of Korea), Norht Korea (Democratic People's Republic of Korea), Mongolia, \" \\\n",
    "    \"Brunei, Cambodia, Indonesia, Laos, Myanmar (Burma), Malaysia, Philippines, Singapore, Thailand, Timor-Leste, Vietnam, Bangladesh, Bhutan,India, Nepal, Guam,\" \\\n",
    "    \"Maldives, Sri Lanka, Australia, New Zealand, Papua New Guinea, Solomon Islands, Nauru, Vanuatu, Fiji, Tonga, Tuvalu, Kiribati, Marshall Islands, Samoa, American Samoa.\" \\\n",
    "    \"NORTHCOM includes Canada, Mexico, Bermuda, Bahamas and the United States.\" \\\n",
    "    \"SOUTHCOM includes Antigua, barbuda, Argentina, Barbados, Belize, Bolivia, Brazil,CHile,Colombia, Costa Rica, Cuba,\" \\\n",
    "    \"Dominica, Dominican Republic, Ecuador, El Salvador, Grenada, Guatemala, Guyana, Haiti, Honduras, Jamaica, \" \\\n",
    "    \"Nicaragua, Panama, Paraguay, Peru, Saint Kitts and Nevis, Saunt Lucia, Saint Vincent and Grenadines, Suriname, Trinidad and Tobago, Uruguay, \" \\\n",
    "    \"Venezuela.\" \\\n",
    "    \"SPACECOM conducts operations in, from and to space to deter conflict and if necessary, defeat aggression, deliver space combat power.\" \\\n",
    "    \"SOCOM develops, and employs, the world's finest SOF to conduct global special operations and activities as part of the Joint Force, in concert with\" \\\n",
    "    \" the U.S. Government Interagency, Allies, and Partners, to support persistent, networked, and distributed combatant command operations and campaigns\" \\\n",
    "    \" against state and non-state actors all to protect and advance U.S. policies and objectives.\" \\\n",
    "    \"STRATCOM is responsible for Strategic Deterrence, Nuclear Operations, Nuclear Command, Control, and Communications (NC3) Enterprise Operations,\" \\\n",
    "    \" Joint Electromagnetic Spectrum Operations, Global Strike and Missile Threat Assessment.\" \\\n",
    "    \"TRANSCOM provides transportation services and capabilities to the other combatant commands, the military services, and defense agencies.\" \\\n",
    "    \" Other: This combatant command is only to be selected if all other combatant commands do not match well to the wargame.\" \\\n",
    "    \"PACOM should be written as INDOPACOM\"\n",
    "    )\n",
    "    category: Wargame_type = Field(description= \"The wargame type limited to policy, capability, conop, process or other.\" \\\n",
    "    \"Policy wargames explore the implications of policy decisions in complex and uncertain environments. These games involve \" \\\n",
    "    \"participants—often experts, officials, or stakeholders—who role-play different actors to test strategies, forecast outcomes,\"\n",
    "    \" and identify potential risks and unintended consequences.\" \\\n",
    "    \"Capability wargames are structured simulations used to identify, test, and refine the capabilities that an organization—typically\" \\\n",
    "    \" a military, government agency, or large institution—needs to effectively operate in future scenarios. These wargames focus less\" \\\n",
    "    \" on specific policy decisions and more on the tools, technologies, personnel, infrastructure, and systems required to achieve known strategic objectives.\" \\\n",
    "    \"Conop wargames are defined as scenario-driven event designed to explore and validate how an organization will employ its forces, capabilities, or resources to \" \\\n",
    "    \"accomplish strategic objectives under realistic and often adversarial conditions. Conop wargames test operational concepts in simulated real-world or \" \\\n",
    "    \"future environments.\" \\\n",
    "    \"Process wargames are secanrio-driven exercises designed to test, evaluate, and improve the internal processes, workflows, coordination mechanisms, and \" \\\n",
    "    \"organizational structures of an entity—such as a military unit or government agency under realistic or stressful conditions.\" \\\n",
    "    \"If no wargame type fits the policy, capabilty, conop, or process wargame definitions, choose other.\")\n",
    "    purpose: str = Field(description=\"Brief summary of the purpose of the wargame, no more than a paragraph long\")\n",
    "\n",
    "\n",
    "class SearchSchema2(BaseModel):\n",
    "    keywords: List[str] = Field(description=\"This is a list of 10-20 keywords in the chunk.\")\n",
    "    country: List[str] = Field(description=\"This is a list of countries in the chunk. This can be an empyt list if no country is mentioned in the chunk.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"COCOM\": {\"enum\": [\"INDOPACOM\", \"PACOM\", \"CENTCOM\", \"EUCOM\", \"AFRICOM\", \"NORTHCOM\", \"SOUTHCOM\", \"SPACECOM\", \"CYBERCOM\", \"SOCOM\", \"STRATCOM\", \"TRANSCOM\", \"Other\"], \"title\": \"COCOM\", \"type\": \"string\"}, \"Domain\": {\"enum\": [\"Land\", \"Maritime\", \"Air\", \"Space\", \"Cyberspace\", \"Other\"], \"title\": \"Domain\", \"type\": \"string\"}, \"Wargame_type\": {\"enum\": [\"policy\", \"capability\", \"conop\", \"process\", \"other\"], \"title\": \"Wargame_type\", \"type\": \"string\"}}, \"properties\": {\"title\": {\"description\": \"the title of the wargame paper\", \"title\": \"Title\", \"type\": \"string\"}, \"date_of_pub\": {\"description\": \"the date the paper was written, if only a year is found, use January 1st. If only a month and year is found, use the 1st of the month.\", \"example\": [\"1999-12-01\", \"2001-01-30\", \"2010-04-15\"], \"format\": \"date\", \"title\": \"Date Of Pub\", \"type\": \"string\"}, \"agency\": {\"description\": \"the list of agencies that either sponsored or hosted the wargame\", \"items\": {\"type\": \"string\"}, \"title\": \"Agency\", \"type\": \"array\"}, \"domain\": {\"description\": \"the list of domains covered in the wargame, these are limited to Land, Maritime, Air, Space and Cyberspace. An other category is left for any wargame that doesnot fit one of the previous listed domains. Land: This domain encompasses the surface of the Earth, excluding the high-water mark, and includes land-based operations. Maritime: This domain includes oceans, seas, bays, estuaries, islands, and coastal areas, along with the airspace above these areas. Air: This domain refers to the atmosphere, extending from the Earth's surface to the altitude where its effects on operations become negligible. Space: This domain includes the area above the atmosphere where the effects of atmospheric drag on airborne objects become negligible. Cyberspace: This domain encompasses the interdependent networks of information technology infrastructures, the Internet, and other data, including computer systems and embedded processors. Other: This domain is only to be selected if all other domains do not match well to the wargame.\", \"items\": {\"$ref\": \"#/$defs/Domain\"}, \"title\": \"Domain\", \"type\": \"array\"}, \"country\": {\"description\": \"This is a list of participating countries or countries that are part of the wargame\", \"items\": {\"type\": \"string\"}, \"title\": \"Country\", \"type\": \"array\"}, \"cocom\": {\"description\": \"the list of combatant commands associated with the wargame, these are limited to AFRICOM, CENTCOM, CYBERCOM, EUCOM, INDOPACOM, NORTHCOM, SPACECOM, SOUTHCOM, STRATCOM, and TRANSCOM. An other category is left for any wargame that doesnot fit one of the previous listed combatant commands.AFRICOM includes all countries in Africa accept Egypt.CENTCOM is includes Egypt, Yemen, Oman, United Arab Emirates, Qatar, Bahrain, Saudi Arabia, Jordan, Israel, Lebanon, Syria, Iraq, Kuwait, Iran, Afganistan, Pakistan, Turkmenistan, Tajikistan, Uzbekistan, Kyrgyzstan and Kazakhstan.CYBRECOM defends the United States, countering hostile cyber actors alongside our interagency, industry, and international partners. Thic COCOM is focused on cyberspace.EUCOM is responsible for military operations in Europe and parts of Eurasia. EUCOM includes the following countries: Albania, Andorra, Austria, Belgium, Bosnia and Herzegovina, BulgariaCroatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Iceland, Ireland, Italy, Kosovo, Latvia, Liechtenstein, Lithuania, Luxembourg, Malta, Moldova, Monaco, Montenegro, Netherlands, North Macedonia, Norway, Poland, Portugal, Romania, San Marino, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Ukraine, United Kingdom, Vatican City. EUCOM also includes the ArticINDOPACOM includes 38 countries: China, Japan, Taiwan, South Korea (Republic of Korea), Norht Korea (Democratic People's Republic of Korea), Mongolia, Brunei, Cambodia, Indonesia, Laos, Myanmar (Burma), Malaysia, Philippines, Singapore, Thailand, Timor-Leste, Vietnam, Bangladesh, Bhutan,India, Nepal, Guam,Maldives, Sri Lanka, Australia, New Zealand, Papua New Guinea, Solomon Islands, Nauru, Vanuatu, Fiji, Tonga, Tuvalu, Kiribati, Marshall Islands, Samoa, American Samoa.NORTHCOM includes Canada, Mexico, Bermuda, Bahamas and the United States.SOUTHCOM includes Antigua, barbuda, Argentina, Barbados, Belize, Bolivia, Brazil,CHile,Colombia, Costa Rica, Cuba,Dominica, Dominican Republic, Ecuador, El Salvador, Grenada, Guatemala, Guyana, Haiti, Honduras, Jamaica, Nicaragua, Panama, Paraguay, Peru, Saint Kitts and Nevis, Saunt Lucia, Saint Vincent and Grenadines, Suriname, Trinidad and Tobago, Uruguay, Venezuela.SPACECOM conducts operations in, from and to space to deter conflict and if necessary, defeat aggression, deliver space combat power.SOCOM develops, and employs, the world's finest SOF to conduct global special operations and activities as part of the Joint Force, in concert with the U.S. Government Interagency, Allies, and Partners, to support persistent, networked, and distributed combatant command operations and campaigns against state and non-state actors all to protect and advance U.S. policies and objectives.STRATCOM is responsible for Strategic Deterrence, Nuclear Operations, Nuclear Command, Control, and Communications (NC3) Enterprise Operations, Joint Electromagnetic Spectrum Operations, Global Strike and Missile Threat Assessment.TRANSCOM provides transportation services and capabilities to the other combatant commands, the military services, and defense agencies. Other: This combatant command is only to be selected if all other combatant commands do not match well to the wargame.PACOM should be written as INDOPACOM\", \"items\": {\"$ref\": \"#/$defs/COCOM\"}, \"title\": \"Cocom\", \"type\": \"array\"}, \"category\": {\"$ref\": \"#/$defs/Wargame_type\", \"description\": \"The wargame type limited to policy, capability, conop, process or other.Policy wargames explore the implications of policy decisions in complex and uncertain environments. These games involve participants—often experts, officials, or stakeholders—who role-play different actors to test strategies, forecast outcomes, and identify potential risks and unintended consequences.Capability wargames are structured simulations used to identify, test, and refine the capabilities that an organization—typically a military, government agency, or large institution—needs to effectively operate in future scenarios. These wargames focus less on specific policy decisions and more on the tools, technologies, personnel, infrastructure, and systems required to achieve known strategic objectives.Conop wargames are defined as scenario-driven event designed to explore and validate how an organization will employ its forces, capabilities, or resources to accomplish strategic objectives under realistic and often adversarial conditions. Conop wargames test operational concepts in simulated real-world or future environments.Process wargames are secanrio-driven exercises designed to test, evaluate, and improve the internal processes, workflows, coordination mechanisms, and organizational structures of an entity—such as a military unit or government agency under realistic or stressful conditions.If no wargame type fits the policy, capabilty, conop, or process wargame definitions, choose other.\"}, \"purpose\": {\"description\": \"Brief summary of the purpose of the wargame, no more than a paragraph long\", \"title\": \"Purpose\", \"type\": \"string\"}}, \"required\": [\"title\", \"date_of_pub\", \"agency\", \"domain\", \"country\", \"cocom\", \"category\", \"purpose\"]}\n",
      "```\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"keywords\": {\"description\": \"This is a list of 10-20 keywords in the chunk.\", \"items\": {\"type\": \"string\"}, \"title\": \"Keywords\", \"type\": \"array\"}, \"country\": {\"description\": \"This is a list of countries in the chunk. This can be an empyt list if no country is mentioned in the chunk.\", \"items\": {\"type\": \"string\"}, \"title\": \"Country\", \"type\": \"array\"}}, \"required\": [\"keywords\", \"country\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=SearchSchema)\n",
    "format_instructions = pydantic_parser.get_format_instructions()\n",
    "\n",
    "# The Pydantic model creates the formatting instructions to be included in the prompt\n",
    "# Here is the what those instructions look like\n",
    "print(format_instructions)\n",
    "\n",
    "\n",
    "pydantic_parser2 = PydanticOutputParser(pydantic_object=SearchSchema2)\n",
    "format_instructions2 = pydantic_parser2.get_format_instructions()\n",
    "\n",
    "# The Pydantic model creates the formatting instructions to be included in the prompt\n",
    "# Here is the what those instructions look like\n",
    "print(format_instructions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b3e5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert wargame reviewer. Analyze the following wargame report in detail.\n",
    "        Break down each major metadata parameter from the format instructions, provide a brief summary.\n",
    "        \n",
    "        Wargame Report: {wargame_report}\n",
    "        \n",
    "        {format_instructions}\n",
    "        \n",
    "        Analyze the wargame following the exact format specified above.\n",
    "        \"\"\",\n",
    "        input_variables=[\"wargame_report\"],\n",
    "        partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    "    )\n",
    "    \n",
    "chain = API_prompt | llm | pydantic_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14171f",
   "metadata": {},
   "source": [
    "### Step 4: Iterate through every document and store in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee5fa631",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",          # Split along line breaks\n",
    "    chunk_size=1500,          # Up to 500 characters per chunk\n",
    "    chunk_overlap=200        # Optional: adds continuity between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e820d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of paths to md files in specified directory\n",
    "directory = \"./final_docling_output\"\n",
    "doc_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if filename.endswith('.md')]\n",
    "\n",
    "## MArcus used unstructure loader...why?\n",
    "docs = [TextLoader(doc_path).load() for doc_path in doc_paths]\n",
    "docs_flat_full = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66dbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working documents: 0 to 3\n",
      "Starting on ./final_docling_output/(U)_DON_Climate_Action_2030_TTX_III_Final_Report.md document\n",
      "Starting on ./final_docling_output/(U)_China_Future_report_AWC_15_May_15.md document\n",
      "Starting on ./final_docling_output/(U)_ACV11_QuickLook_Final.md document\n",
      "Starting on ./final_docling_output/(U)_ACV11_Final_Report.md document\n",
      "\n",
      "Loaded 0 existing IDs from LanceDB.\n",
      "Created 170 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 6/6 [02:39<00:00, 26.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 170\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 170\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 4 to 7\n",
      "Starting on ./final_docling_output/(U)_EW09_Quicklook.md document\n",
      "Starting on ./final_docling_output/(U)_EW09_Final_Report.md document\n",
      "Starting on ./final_docling_output/(U)_EW08_Quicklook.md document\n",
      "Starting on ./final_docling_output/(U)_EW08_Final_Report.md document\n",
      "\n",
      "Loaded 170 existing IDs from LanceDB.\n",
      "Created 174 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 6/6 [02:44<00:00, 27.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 174\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 174\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 8 to 11\n",
      "Starting on ./final_docling_output/(U)_Syria_Analysis_Game_Final_Report.md document\n",
      "Starting on ./final_docling_output/(U)_Expeditionary_Medicine_2015_Final_Report.md document\n",
      "Starting on ./final_docling_output/(U)_EW11_Final_Report.md document\n",
      "Starting on ./final_docling_output/(U)_EW10_Final_Report.md document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1507, which is longer than the specified 1500\n",
      "Created a chunk of size 1563, which is longer than the specified 1500\n",
      "Created a chunk of size 1673, which is longer than the specified 1500\n",
      "Created a chunk of size 1584, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 344 existing IDs from LanceDB.\n",
      "Created 337 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11/11 [05:07<00:00, 27.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 337\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 337\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 12 to 15\n",
      "Starting on ./final_docling_output/ADA542668.md document\n",
      "Starting on ./final_docling_output/2013_Naval_Services_Game_Report.md document\n",
      "Starting on ./final_docling_output/2010_MAD_Game_Part1.md document\n",
      "Starting on ./final_docling_output/1994_Russian-UK-US.md document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1725, which is longer than the specified 1500\n",
      "Created a chunk of size 1672, which is longer than the specified 1500\n",
      "Created a chunk of size 2178, which is longer than the specified 1500\n",
      "Created a chunk of size 1736, which is longer than the specified 1500\n",
      "Created a chunk of size 1616, which is longer than the specified 1500\n",
      "Created a chunk of size 1867, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 681 existing IDs from LanceDB.\n",
      "Created 438 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 14/14 [06:32<00:00, 28.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 438\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 438\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 16 to 19\n",
      "Starting on ./final_docling_output/Algernon_Wargame.md document\n",
      "Starting on ./final_docling_output/After_Action_Report_JCRX_FLINTLOCK_86.md document\n",
      "Starting on ./final_docling_output/Adding_Weather_to_Wargames.md document\n",
      "Starting on ./final_docling_output/Addendum_to_ARL-TR-4005.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1660, which is longer than the specified 1500\n",
      "Created a chunk of size 1660, which is longer than the specified 1500\n",
      "Created a chunk of size 1729, which is longer than the specified 1500\n",
      "Created a chunk of size 1746, which is longer than the specified 1500\n",
      "Created a chunk of size 1522, which is longer than the specified 1500\n",
      "Created a chunk of size 1514, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1119 existing IDs from LanceDB.\n",
      "Created 292 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [05:14<00:00, 31.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 292\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 292\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 20 to 23\n",
      "Starting on ./final_docling_output/ARDE_WarGame_1960.md document\n",
      "Starting on ./final_docling_output/Architecture_Tradeoff_Analysis_2001.md document\n",
      "Starting on ./final_docling_output/All_Hazards_Plan_Validation_Table_Top_Exercise-ARA.md document\n",
      "Starting on ./final_docling_output/All_Hazards_After_Action_Report.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1535, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1411 existing IDs from LanceDB.\n",
      "Created 105 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 4/4 [01:41<00:00, 25.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 105\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 105\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 24 to 27\n",
      "Starting on ./final_docling_output/Battle_Simulations.md document\n",
      "Starting on ./final_docling_output/Battle_of_Jutland_1922_2020.md document\n",
      "Starting on ./final_docling_output/A_Report_by_the_Military_Committee_on_NATO_EXERCISES_1959.md document\n",
      "Starting on ./final_docling_output/Auger_1997.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1682, which is longer than the specified 1500\n",
      "Created a chunk of size 1522, which is longer than the specified 1500\n",
      "Created a chunk of size 1618, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1516 existing IDs from LanceDB.\n",
      "Created 147 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 5/5 [02:15<00:00, 27.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 147\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 147\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 28 to 31\n",
      "Starting on ./final_docling_output/CHENNAULT_EVENT.md document\n",
      "Starting on ./final_docling_output/Caspian_Sea_Game_1998.md document\n",
      "Starting on ./final_docling_output/CARMAX_83A_Research_Project.md document\n",
      "too many tokens, attempting to shorten document with 576622 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/CAMMS_v_CPX.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/CAMMS_v_CPX.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"EVALUATION OF A COMPUTER-ASSISTED BATTLE SIMULATION: CAMMS VERSUS A CPX\", \"date_of_pub\": \"1979-04-01\", \"agency\": [\"U.S. Army Research Institute for the Behavioral and Social Sciences\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"TRADOC\"], \"category\": \"capability\", \"purpose\": \"The purpose of this investigation was to evaluate the cost and training effectiveness of the Computer-Assisted Map Maneuver System (CAMMS) in comparison to a conventional command post exercise (CPX).\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='TRADOC', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1888, which is longer than the specified 1500\n",
      "Created a chunk of size 1750, which is longer than the specified 1500\n",
      "Created a chunk of size 1544, which is longer than the specified 1500\n",
      "Created a chunk of size 1517, which is longer than the specified 1500\n",
      "Created a chunk of size 1667, which is longer than the specified 1500\n",
      "Created a chunk of size 1701, which is longer than the specified 1500\n",
      "Created a chunk of size 1744, which is longer than the specified 1500\n",
      "Created a chunk of size 1804, which is longer than the specified 1500\n",
      "Created a chunk of size 2220, which is longer than the specified 1500\n",
      "Created a chunk of size 1532, which is longer than the specified 1500\n",
      "Created a chunk of size 1772, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1663 existing IDs from LanceDB.\n",
      "Created 506 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 16/16 [10:09<00:00, 38.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 506\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 506\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 32 to 35\n",
      "Starting on ./final_docling_output/Counter-Insurgency_Study.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Counter-Insurgency_Study.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"AGILE-COIN GAME\", \"date_of_pub\": \"1965-11-01\", \"agency\": [\"ARPA\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"INDOPACOM\"], \"category\": [\"capability\"], \"purpose\": \"The purpose of this wargame is to explore the feasibility of computer models based on game findings that imitate some of the major aspects of the terror-phase of internal revolutionary conflict.\"}. Got: 1 validation error for SearchSchema\n",
      "category\n",
      "  Input should be 'policy', 'capability', 'conop', 'process' or 'other' [type=enum, input_value=['capability'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/Commercial_Satellite_Report.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Commercial_Satellite_Report.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 443df5fa-a61e-49cb-9144-b7e6e18e11ef)')\n",
      "Starting on ./final_docling_output/CIVLOG_69_Exercise.md document\n",
      "Starting on ./final_docling_output/China_Future_Report_AWC_2015.md document\n",
      "\n",
      "Loaded 2169 existing IDs from LanceDB.\n",
      "Created 39 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 2/2 [00:40<00:00, 20.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 39\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 39\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 36 to 39\n",
      "Starting on ./final_docling_output/DSB_report_2021.md document\n",
      "Starting on ./final_docling_output/DoD_Activities_1989.md document\n",
      "Starting on ./final_docling_output/document.md document\n",
      "Starting on ./final_docling_output/Defend_Forward_Game_Report_2019.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2223, which is longer than the specified 1500\n",
      "Created a chunk of size 1503, which is longer than the specified 1500\n",
      "Created a chunk of size 1611, which is longer than the specified 1500\n",
      "Created a chunk of size 1743, which is longer than the specified 1500\n",
      "Created a chunk of size 1588, which is longer than the specified 1500\n",
      "Created a chunk of size 1573, which is longer than the specified 1500\n",
      "Created a chunk of size 2488, which is longer than the specified 1500\n",
      "Created a chunk of size 2418, which is longer than the specified 1500\n",
      "Created a chunk of size 1875, which is longer than the specified 1500\n",
      "Created a chunk of size 1875, which is longer than the specified 1500\n",
      "Created a chunk of size 1875, which is longer than the specified 1500\n",
      "Created a chunk of size 1875, which is longer than the specified 1500\n",
      "Created a chunk of size 1875, which is longer than the specified 1500\n",
      "Created a chunk of size 1728, which is longer than the specified 1500\n",
      "Created a chunk of size 1565, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2208 existing IDs from LanceDB.\n",
      "Created 303 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [04:37<00:00, 27.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 303\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 303\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 40 to 43\n",
      "Starting on ./final_docling_output/Exhibit_R-2_Iron_Crucible.md document\n",
      "Starting on ./final_docling_output/Exercise_Cygnus_Report.md document\n",
      "Starting on ./final_docling_output/ExerciseViking22_Final_Report.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/ExerciseViking22_Final_Report.md\n",
      "error code: Failed to parse SearchSchema from completion {\"Title\": \"VIKING 22\", \"DateOfPub\": \"2022-03-20\", \"Agency\": [\"NATO\", \"UN\"], \"Domain\": [\"Land\", \"Maritime\", \"Air\", \"Space\", \"Cyberspace\"], \"Country\": [\"Sweden\", \"Finland\", \"Brazil\", \"Southland\"], \"Cocom\": [\"EUCOM\", \"NORTHCOM\", \"INDOPACOM\", \"CENTCOM\"], \"Category\": \"conop\", \"Purpose\": \"Exercise Viking is an important exercise and a great vantage between Military, Civilian and police. This exercise will help to expand the horizon of knowledge and experience of all the participants for their future tasks in any real crisis.\"}. Got: 8 validation errors for SearchSchema\n",
      "title\n",
      "  Field required [type=missing, input_value={'Title': 'VIKING 22', 'D...ks in any real crisis.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "date_of_pub\n",
      "  Field required [type=missing, input_value={'Title': 'VIKING 22', 'D...ks in any real crisis.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "agency\n",
      "  Field required [type=missing, input_value={'Title': 'VIKING 22', 'D...ks in any real crisis.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "domain\n",
      "  Field required [type=missing, input_value={'Title': 'VIKING 22', 'D...ks in any real crisis.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "country\n",
      "  Field required [type=missing, input_value={'Title': 'VIKING 22', 'D...ks in any real crisis.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "cocom\n",
      "  Field required [type=missing, input_value={'Title': 'VIKING 22', 'D...ks in any real crisis.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "category\n",
      "  Field required [type=missing, input_value={'Title': 'VIKING 22', 'D...ks in any real crisis.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "purpose\n",
      "  Field required [type=missing, input_value={'Title': 'VIKING 22', 'D...ks in any real crisis.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/DSTO_2003.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1688, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 8149, which is longer than the specified 1500\n",
      "Created a chunk of size 1523, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2511 existing IDs from LanceDB.\n",
      "Created 309 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [04:12<00:00, 25.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 309\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 309\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 44 to 47\n",
      "Starting on ./final_docling_output/FMD_TTX_ESF11_Report.md document\n",
      "Starting on ./final_docling_output/Fleet_Arctic_Operations_Game.md document\n",
      "Starting on ./final_docling_output/Final_Production_2001_2019.md document\n",
      "too many tokens, attempting to shorten document with 621940 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/Falklands_Wargame.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2656, which is longer than the specified 1500\n",
      "Created a chunk of size 2613, which is longer than the specified 1500\n",
      "Created a chunk of size 1764, which is longer than the specified 1500\n",
      "Created a chunk of size 1764, which is longer than the specified 1500\n",
      "Created a chunk of size 1626, which is longer than the specified 1500\n",
      "Created a chunk of size 1703, which is longer than the specified 1500\n",
      "Created a chunk of size 1866, which is longer than the specified 1500\n",
      "Created a chunk of size 1661, which is longer than the specified 1500\n",
      "Created a chunk of size 1530, which is longer than the specified 1500\n",
      "Created a chunk of size 1800, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2820 existing IDs from LanceDB.\n",
      "Created 741 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 24/24 [13:07<00:00, 32.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 741\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 741\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 48 to 51\n",
      "Starting on ./final_docling_output/Game_Report_Global08.md document\n",
      "Starting on ./final_docling_output/FY03_Wargaming_Assessment_Report.md document\n",
      "Starting on ./final_docling_output/Future_Warfare_20XX_Wargame_Series_Report.md document\n",
      "Starting on ./final_docling_output/Framework_for_MNE_5.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1621, which is longer than the specified 1500\n",
      "Created a chunk of size 1680, which is longer than the specified 1500\n",
      "Created a chunk of size 1626, which is longer than the specified 1500\n",
      "Created a chunk of size 1506, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3561 existing IDs from LanceDB.\n",
      "Created 474 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 15/15 [07:05<00:00, 28.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 474\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 474\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 52 to 55\n",
      "Starting on ./final_docling_output/Global_Wargame_2000.md document\n",
      "Starting on ./final_docling_output/Global_Shipping_Game_2010.md document\n",
      "Starting on ./final_docling_output/Globally_Integrated_Logistics_2017.md document\n",
      "time out error, pausing for 10 seconds\n",
      "Starting on ./final_docling_output/GAMMA_MNE_4.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1627, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4035 existing IDs from LanceDB.\n",
      "Created 459 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 15/15 [06:55<00:00, 27.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 459\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 459\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 56 to 59\n",
      "Starting on ./final_docling_output/GridEx_I_Report.md document\n",
      "Starting on ./final_docling_output/GridEx_IV_Report.md document\n",
      "Starting on ./final_docling_output/GridEx_II_Report.md document\n",
      "Starting on ./final_docling_output/GridEx_III_Report.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1522, which is longer than the specified 1500\n",
      "Created a chunk of size 1606, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4494 existing IDs from LanceDB.\n",
      "Created 233 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 8/8 [03:20<00:00, 25.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 233\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 233\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 60 to 63\n",
      "Starting on ./final_docling_output/Information_Flow_1982.md document\n",
      "Starting on ./final_docling_output/GridEx_V_Report.md document\n",
      "Starting on ./final_docling_output/GridEx_VI_Report.md document\n",
      "Starting on ./final_docling_output/GridEx_VII_Report.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1574, which is longer than the specified 1500\n",
      "Created a chunk of size 1725, which is longer than the specified 1500\n",
      "Created a chunk of size 2580, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4727 existing IDs from LanceDB.\n",
      "Created 260 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 9/9 [04:27<00:00, 29.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 260\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 260\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 64 to 67\n",
      "Starting on ./final_docling_output/Joint_Staff_OrgChart_4-27-23.md document\n",
      "Starting on ./final_docling_output/Issues_Secretary_of_Navy_Wargame_94.md document\n",
      "Starting on ./final_docling_output/Irregular_Challenges_2010.md document\n",
      "Starting on ./final_docling_output/Inter-American_Game_Report.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1556, which is longer than the specified 1500\n",
      "Created a chunk of size 1589, which is longer than the specified 1500\n",
      "Created a chunk of size 1828, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4987 existing IDs from LanceDB.\n",
      "Created 291 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [05:14<00:00, 31.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 291\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 291\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 68 to 71\n",
      "Starting on ./final_docling_output/Kuznetsov_13_Game_Report.md document\n",
      "Starting on ./final_docling_output/Kuznetsov_11_Game_Report.md document\n",
      "Starting on ./final_docling_output/KIBOWI_Netherlands_Army.md document\n",
      "Starting on ./final_docling_output/Key_NATO_Exercises_2021.md document\n",
      "\n",
      "Loaded 5278 existing IDs from LanceDB.\n",
      "Created 97 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 4/4 [01:28<00:00, 22.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 97\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 97\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 72 to 75\n",
      "Starting on ./final_docling_output/Manual_FoodSafety_Slovak_2017.md document\n",
      "Starting on ./final_docling_output/Manhattan_2001.md document\n",
      "Starting on ./final_docling_output/Mali_Analysis_Strategic_Wargaming_Series.md document\n",
      "Starting on ./final_docling_output/LOGWAR_15_Analysis_Report.md document\n",
      "time out error, pausing for 10 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2017, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5375 existing IDs from LanceDB.\n",
      "Created 306 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [05:30<00:00, 33.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 306\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 306\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 76 to 79\n",
      "Starting on ./final_docling_output/ME7_Deterrence.md document\n",
      "Starting on ./final_docling_output/ME7_Access_to_Space.md document\n",
      "Starting on ./final_docling_output/Maritime_Stability_Operations_2011.md document\n",
      "Starting on ./final_docling_output/Maritime_Domain_Awareness_Operational_Game_2010.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1539, which is longer than the specified 1500\n",
      "Created a chunk of size 1770, which is longer than the specified 1500\n",
      "Created a chunk of size 1778, which is longer than the specified 1500\n",
      "Created a chunk of size 1636, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5681 existing IDs from LanceDB.\n",
      "Created 390 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 13/13 [05:40<00:00, 26.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 390\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 390\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 80 to 83\n",
      "Starting on ./final_docling_output/ME7_Outcome_1_092012.md document\n",
      "Starting on ./final_docling_output/ME7_Mediterranean.md document\n",
      "Starting on ./final_docling_output/ME7_Maritime_Security_Arctic.md document\n",
      "Starting on ./final_docling_output/ME7_Diagrams.md document\n",
      "\n",
      "Loaded 6071 existing IDs from LanceDB.\n",
      "Created 227 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 8/8 [03:17<00:00, 24.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 227\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 227\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 84 to 87\n",
      "Starting on ./final_docling_output/ME7_Space_Dependencies.md document\n",
      "Starting on ./final_docling_output/ME7_Outcome_4__2013.md document\n",
      "Starting on ./final_docling_output/ME7_Outcome_3_Cyber_2012.md document\n",
      "Starting on ./final_docling_output/ME7_Outcome_3.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1565, which is longer than the specified 1500\n",
      "Created a chunk of size 1904, which is longer than the specified 1500\n",
      "Created a chunk of size 2314, which is longer than the specified 1500\n",
      "Created a chunk of size 1510, which is longer than the specified 1500\n",
      "Created a chunk of size 1967, which is longer than the specified 1500\n",
      "Created a chunk of size 1535, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6298 existing IDs from LanceDB.\n",
      "Created 308 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [04:34<00:00, 27.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 308\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 308\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 88 to 91\n",
      "Starting on ./final_docling_output/MNE_5_Findings.md document\n",
      "Starting on ./final_docling_output/MNE_5_Cooperative.md document\n",
      "Starting on ./final_docling_output/MNE4_Modeling_and_Simulation.md document\n",
      "Starting on ./final_docling_output/Memo_31_081956.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1632, which is longer than the specified 1500\n",
      "Created a chunk of size 1903, which is longer than the specified 1500\n",
      "Created a chunk of size 1620, which is longer than the specified 1500\n",
      "Created a chunk of size 2148, which is longer than the specified 1500\n",
      "Created a chunk of size 1698, which is longer than the specified 1500\n",
      "Created a chunk of size 1686, which is longer than the specified 1500\n",
      "Created a chunk of size 1693, which is longer than the specified 1500\n",
      "Created a chunk of size 1616, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6606 existing IDs from LanceDB.\n",
      "Created 124 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 4/4 [01:47<00:00, 26.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 124\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 124\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 92 to 95\n",
      "Starting on ./final_docling_output/Naval_Mine_Anti-Submarine_2013.md document\n",
      "Starting on ./final_docling_output/NATO_EXERCISES_1959.md document\n",
      "Starting on ./final_docling_output/MNE_7_Product_Catalogue.md document\n",
      "Starting on ./final_docling_output/MNE_5_Results_and_Products.md document\n",
      "\n",
      "Loaded 6730 existing IDs from LanceDB.\n",
      "Created 97 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 4/4 [01:32<00:00, 23.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 97\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 97\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 96 to 99\n",
      "Starting on ./final_docling_output/Navy-Private_Sector_Critical_Infrastructure_2017.md document\n",
      "Starting on ./final_docling_output/Naval_Services_Game_12.md document\n",
      "Starting on ./final_docling_output/Naval_Services_Game13_Game_Report.md document\n",
      "Starting on ./final_docling_output/Naval_Services_Game13_Analytic_Summary.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1961, which is longer than the specified 1500\n",
      "Created a chunk of size 1895, which is longer than the specified 1500\n",
      "Created a chunk of size 2045, which is longer than the specified 1500\n",
      "Created a chunk of size 1616, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6827 existing IDs from LanceDB.\n",
      "Created 439 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 14/14 [06:11<00:00, 26.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 439\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 439\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 100 to 103\n",
      "Starting on ./final_docling_output/Olympiad_I-62.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Olympiad_I-62.md\n",
      "error code: Invalid json output: Here is a JSON-formatted version of your requested output based on the provided schema:\n",
      "\n",
      " {\n",
      "  \"Title\": \"OLYMPIAD I-62\",\n",
      "  \"Date Of Pub\": \"1962-12-01\",\n",
      "  \"Agency\": [\n",
      "    \"Assistant Secretary of Defense for International Security Affairs\"\n",
      "  ],\n",
      "  \"Domain\": [\n",
      "    \"Land\",\n",
      "    \"Maritime\",\n",
      "    \"Air\",\n",
      "    \"Space\",\n",
      "    \"Cyberspace\"\n",
      "  ],\n",
      "  \"Country\": [\n",
      "    \"United States\",\n",
      "    \"United Kingdom\"\n",
      "  ],\n",
      "  \"Cocom\": [\n",
      "    \"INDOPACOM\"\n",
      "  ],\n",
      "  \"Category\": \"Policy\",\n",
      "  \"Purpose\": \"To explore and test the implications of different policy decisions and military strategies in a global context\"\n",
      "}\n",
      "\n",
      "Note that I used examples provided in the schema to fill the fields. If you need any adjustments or have any questions, please let me know.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/ODNA_Transformation_Strategy_GameIV.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/ODNA_Transformation_Strategy_GameIV.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 57ed6e8c-8505-4e81-95e1-be9cc53c47ab)')\n",
      "Starting on ./final_docling_output/New_Mexico_Tornado_Table_Top_Exercise_(TTX)_After_Action_Report_(AAR).md document\n",
      "Starting on ./final_docling_output/Navy_Irregular_Challenges_Game_10.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1556, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7266 existing IDs from LanceDB.\n",
      "Created 124 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 4/4 [01:47<00:00, 26.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 124\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 124\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 104 to 107\n",
      "Starting on ./final_docling_output/paper_370.md document\n",
      "error at ./final_docling_output/paper_370.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Tactical Engagement Simulation Training: A Method for Learning the Realities of Combat\", \"date_of_pub\": \"1979-08-01\", \"agency\": [\"U.S. Army Research Institute for the Behavioral and Social Sciences\", \"U.S. Army Training Support Center\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"TRADOC\"], \"category\": \"process\", \"purpose\": \"The purpose of this wargame report is to describe a method for collective training for combat arms units, known as engagement simulation, which provides an environment for the training of tactical skills for a complete unit. The report also discusses a method for empirically determining which tactical behaviors are related to successful mission performance.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='TRADOC', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/Pacific_Partners_Wargame_Analysis.md document\n",
      "Starting on ./final_docling_output/Op_Tay_181400CMAR04.md document\n",
      "Starting on ./final_docling_output/Operation_Iraqi_Freedom_History_Brief.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1668, which is longer than the specified 1500\n",
      "Created a chunk of size 1724, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7390 existing IDs from LanceDB.\n",
      "Created 65 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 3/3 [01:05<00:00, 21.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 65\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 65\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 108 to 111\n",
      "Starting on ./final_docling_output/Post-2014_Afghanistan_Wargame_Analysis.md document\n",
      "Starting on ./final_docling_output/PORTABLE_CASTLE_Quick_Look_Final_20180601.md document\n",
      "Starting on ./final_docling_output/POLITICA_and_AGILE-COIN_1966.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/POLITICA_and_AGILE-COIN_1966.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"DEMONSTRATION OF TWO SIMULATIONS OF INTERNAL REVOLUTIONARY CONFLICT: POLITICA AND AGILE-COIN\", \"date_of_pub\": \"1967-01-01\", \"agency\": [\"Abt Associates Inc.\"], \"domain\": [\"Land\", \"Cyberspace\"], \"country\": [\"Inertia\"], \"cocom\": [\"Other\"], \"category\": \"Policy\", \"purpose\": \"To simulate and understand the dynamics of internal conflict, specifically revolutionary conflict, and how different factions interact and influence outcomes in such scenarios.\"}. Got: 1 validation error for SearchSchema\n",
      "category\n",
      "  Input should be 'policy', 'capability', 'conop', 'process' or 'other' [type=enum, input_value='Policy', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/paper_381.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2124, which is longer than the specified 1500\n",
      "Created a chunk of size 1595, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7455 existing IDs from LanceDB.\n",
      "Created 133 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 5/5 [01:54<00:00, 22.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 133\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 133\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 112 to 115\n",
      "Starting on ./final_docling_output/PROSAIC TWAIN_Quick Look_V4.md document\n",
      "Starting on ./final_docling_output/Proliferation_Security_Initiative_14.md document\n",
      "Starting on ./final_docling_output/Project_Centaur__The_Wargame_Rules.md document\n",
      "Starting on ./final_docling_output/PRESCIENT VISION Quick Look FINAL.md document\n",
      "\n",
      "Loaded 7588 existing IDs from LanceDB.\n",
      "Created 286 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 9/9 [05:19<00:00, 35.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 286\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 286\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 116 to 119\n",
      "Starting on ./final_docling_output/report_1164.md document\n",
      "Starting on ./final_docling_output/Red_Gaming_in_Support_of_the_War_on_Terrorism_Sandia_Red_Game_Report.md document\n",
      "Starting on ./final_docling_output/QUICK.md document\n",
      "Starting on ./final_docling_output/QDR_Defense_Strategy.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 1768, which is longer than the specified 1500\n",
      "Created a chunk of size 1746, which is longer than the specified 1500\n",
      "Created a chunk of size 1613, which is longer than the specified 1500\n",
      "Created a chunk of size 1591, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7874 existing IDs from LanceDB.\n",
      "Created 407 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  38%|███▊      | 5/13 [04:35<09:19, 69.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/QUICK.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: a6d8ea63-9866-4470-bce3-7b104a2852dc)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 13/13 [09:30<00:00, 43.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 406\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 1\n",
      "Total chunked_docs   : 407\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 120 to 123\n",
      "Starting on ./final_docling_output/Report_of_International_Game_96.md document\n",
      "Starting on ./final_docling_output/Report_NATO_Exercises_1956.md document\n",
      "Starting on ./final_docling_output/Report_IV_Crisis_Sim.md document\n",
      "Starting on ./final_docling_output/Report_Crisis_Simulation_2009__EFSA.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2145, which is longer than the specified 1500\n",
      "Created a chunk of size 1519, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8280 existing IDs from LanceDB.\n",
      "Created 346 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11/11 [05:32<00:00, 30.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 346\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 1\n",
      "Total chunked_docs   : 346\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 124 to 127\n",
      "Starting on ./final_docling_output/Setear_061990.md document\n",
      "Starting on ./final_docling_output/SCYLLA_III-73_Quick_Look.md document\n",
      "too many tokens, attempting to shorten document with 572189 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "Starting on ./final_docling_output/Revolution_in_Military_Affairs_2020_Vol_Il.md document\n",
      "Starting on ./final_docling_output/Report_On_NATO_Exercises.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1888, which is longer than the specified 1500\n",
      "Created a chunk of size 1800, which is longer than the specified 1500\n",
      "Created a chunk of size 1847, which is longer than the specified 1500\n",
      "Created a chunk of size 1825, which is longer than the specified 1500\n",
      "Created a chunk of size 1664, which is longer than the specified 1500\n",
      "Created a chunk of size 1931, which is longer than the specified 1500\n",
      "Created a chunk of size 1813, which is longer than the specified 1500\n",
      "Created a chunk of size 2292, which is longer than the specified 1500\n",
      "Created a chunk of size 1731, which is longer than the specified 1500\n",
      "Created a chunk of size 1512, which is longer than the specified 1500\n",
      "Created a chunk of size 1850, which is longer than the specified 1500\n",
      "Created a chunk of size 3347, which is longer than the specified 1500\n",
      "Created a chunk of size 1600, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8626 existing IDs from LanceDB.\n",
      "Created 736 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  74%|███████▍  | 17/23 [13:20<07:45, 77.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/SCYLLA_III-73_Quick_Look.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 18363a89-b5e8-42cf-b084-ac053736e852)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 23/23 [18:42<00:00, 48.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 735\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 2\n",
      "Total chunked_docs   : 736\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 128 to 131\n",
      "Starting on ./final_docling_output/Six_Demos_AGILE-COIN.md document\n",
      "time out error, pausing for 10 seconds\n",
      "Starting on ./final_docling_output/SIGNAL_Game_Manual.md document\n",
      "Starting on ./final_docling_output/Shipbuilding_Game.md document\n",
      "Starting on ./final_docling_output/Shaken_10-01_Table_Top_Exercise_After_Action_Report.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1513, which is longer than the specified 1500\n",
      "Created a chunk of size 1752, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9361 existing IDs from LanceDB.\n",
      "Created 300 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [04:38<00:00, 27.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 300\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 2\n",
      "Total chunked_docs   : 300\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 132 to 135\n",
      "Starting on ./final_docling_output/Syria_Analysis_Game.md document\n",
      "Starting on ./final_docling_output/Stress_Testing_Financial_Crisis_2007.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Stress_Testing_Financial_Crisis_2007.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: a182a7d2-57f3-4b5e-af15-5d53b097ae71)')\n",
      "Starting on ./final_docling_output/STDE24_Fact_Sheet.md document\n",
      "Starting on ./final_docling_output/SMITE.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n",
      "Created a chunk of size 1769, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9661 existing IDs from LanceDB.\n",
      "Created 211 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 7/7 [03:14<00:00, 27.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 211\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 2\n",
      "Total chunked_docs   : 211\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 136 to 139\n",
      "Starting on ./final_docling_output/URB-COIN_Game.md document\n",
      "Starting on ./final_docling_output/TRIDENT_JUNCTURE_2015.md document\n",
      "Starting on ./final_docling_output/Theater_Battle_Model_Volume_VII.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Theater_Battle_Model_Volume_VII.md\n",
      "error code: Invalid json output: Here is the output that follows the specified schema:\n",
      "\n",
      "\n",
      "{\n",
      "  \"title\": \"Theater Battle Model\",\n",
      "  \"date_of_pub\": \"1968-01-01\",\n",
      "  \"agency\": [\n",
      "    \"Joint War Games Agency\"\n",
      "  ],\n",
      "  \"domain\": {\n",
      "    \"items\": [\n",
      "      \"Land\",\n",
      "      \"Maritime\",\n",
      "      \"Air\",\n",
      "      \"Space\",\n",
      "      \"Cyberspace\"\n",
      "    ]\n",
      "  },\n",
      "  \"country\": [\n",
      "    \"United States\"\n",
      "  ],\n",
      "  \"cocom\": {\n",
      "    \"items\": [\n",
      "      \"PACOM\"\n",
      "    ]\n",
      "  },\n",
      "  \"category\": {\n",
      "    \"$ref\": \"#/$defs/Wargame_type\"\n",
      "  },\n",
      "  \"purpose\": \"The purpose of the wargame is to analyze and evaluate military strategies and tactics in a simulated environment, providing insights into the potential outcomes of different scenarios and the effectiveness of various military operations.\"\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/Tacspiel_War.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1511, which is longer than the specified 1500\n",
      "Created a chunk of size 1756, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9872 existing IDs from LanceDB.\n",
      "Created 315 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [05:19<00:00, 31.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 315\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 2\n",
      "Total chunked_docs   : 315\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 140 to 143\n",
      "Starting on ./final_docling_output/WAGCAP_Phase_II.md document\n",
      "error at ./final_docling_output/WAGCAP_Phase_II.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"IMPROVEMENT OF THE WAR GAMING CAPABILITY, PHASE II (WAGCAP II)\", \"date_of_pub\": \"1973-06-01\", \"agency\": [\"US Army Combat Developments Command\", \"Computer Sciences Corporation\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"USACDC\"], \"category\": \"capability\", \"purpose\": \"The purpose of this wargame was to improve the war gaming capability of the US Army by developing and testing the Division War Game (DIVWAG) model.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='USACDC', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/VISTA QUAKE Quick Look Final.md document\n",
      "Starting on ./final_docling_output/USREDCOM_1974-1978.md document\n",
      "Starting on ./final_docling_output/USAFRICOM_2014.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1528, which is longer than the specified 1500\n",
      "Created a chunk of size 1528, which is longer than the specified 1500\n",
      "Created a chunk of size 1528, which is longer than the specified 1500\n",
      "Created a chunk of size 1523, which is longer than the specified 1500\n",
      "Created a chunk of size 1523, which is longer than the specified 1500\n",
      "Created a chunk of size 1523, which is longer than the specified 1500\n",
      "Created a chunk of size 1523, which is longer than the specified 1500\n",
      "Created a chunk of size 1518, which is longer than the specified 1500\n",
      "Created a chunk of size 1518, which is longer than the specified 1500\n",
      "Created a chunk of size 1518, which is longer than the specified 1500\n",
      "Created a chunk of size 1518, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10187 existing IDs from LanceDB.\n",
      "Created 309 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10/10 [04:48<00:00, 28.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 309\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 2\n",
      "Total chunked_docs   : 309\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 144 to 147\n",
      "Starting on ./final_docling_output/WAGCAP_Phase_II_AppD.md document\n",
      "Starting on ./final_docling_output/WAGCAP_Phase_II_AppC.md document\n",
      "too many tokens, attempting to shorten document with 992921 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "error at ./final_docling_output/WAGCAP_Phase_II_AppC.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"IMPROVEMENT OF THE WAR-GAMING CAPABILITY, PHASE II\", \"date_of_pub\": \"1973-06-01\", \"agency\": [\"Computer Sciences Corporation\", \"US Army\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"USACDC\"], \"category\": \"capability\", \"purpose\": \"The purpose of this wargame is to improve the war-gaming capability of the US Army by modifying the Division War Game (DIVWAG) model to increase its efficiency and acceptability as a division-level war gaming tool.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='USACDC', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md document\n",
      "too many tokens, attempting to shorten document with 894488 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/WAGCAP_Phase_II_AppB_Part_I.md document\n",
      "too many tokens, attempting to shorten document with 999757 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1625, which is longer than the specified 1500\n",
      "Created a chunk of size 2276, which is longer than the specified 1500\n",
      "Created a chunk of size 1637, which is longer than the specified 1500\n",
      "Created a chunk of size 1531, which is longer than the specified 1500\n",
      "Created a chunk of size 1956, which is longer than the specified 1500\n",
      "Created a chunk of size 1710, which is longer than the specified 1500\n",
      "Created a chunk of size 1642, which is longer than the specified 1500\n",
      "Created a chunk of size 1617, which is longer than the specified 1500\n",
      "Created a chunk of size 1732, which is longer than the specified 1500\n",
      "Created a chunk of size 1790, which is longer than the specified 1500\n",
      "Created a chunk of size 1520, which is longer than the specified 1500\n",
      "Created a chunk of size 1689, which is longer than the specified 1500\n",
      "Created a chunk of size 1842, which is longer than the specified 1500\n",
      "Created a chunk of size 1518, which is longer than the specified 1500\n",
      "Created a chunk of size 1540, which is longer than the specified 1500\n",
      "Created a chunk of size 1515, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10496 existing IDs from LanceDB.\n",
      "Created 1641 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  62%|██████▏   | 32/52 [18:25<22:26, 67.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 0e0cd374-f112-495b-9e32-9d85c246c168)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  90%|█████████ | 47/52 [28:36<05:07, 61.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 9c24007b-9c5e-47c8-a3e3-0c5135a0a513)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 52/52 [31:36<00:00, 36.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 1639\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 4\n",
      "Total chunked_docs   : 1641\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 148 to 151\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_VII_DIVWAG.md document\n",
      "too many tokens, attempting to shorten document with 816095 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_IV.md document\n",
      "too many tokens, attempting to shorten document with 601216 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_III_DIVWAG_Tech_Manual.md document\n",
      "too many tokens, attempting to shorten document with 892546 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_I.md document\n",
      "error at ./final_docling_output/WAGCAP_Volume_I.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Improvement of the War Gaming Capability (WAGCAP)\", \"date_of_pub\": \"1972-08-01\", \"agency\": [\"US Army Combat Developments Command\", \"Computer Sciences Corporation\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"OTHER\"], \"category\": \"capability\", \"purpose\": \"The purpose of the WAGCAP project was to improve the war gaming capability of the US Army Combat Developments Command by developing and testing a computer-assisted war game model, known as DIVWAG, and training government personnel in its use.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='OTHER', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2104, which is longer than the specified 1500\n",
      "Created a chunk of size 1717, which is longer than the specified 1500\n",
      "Created a chunk of size 1534, which is longer than the specified 1500\n",
      "Created a chunk of size 1853, which is longer than the specified 1500\n",
      "Created a chunk of size 1826, which is longer than the specified 1500\n",
      "Created a chunk of size 1613, which is longer than the specified 1500\n",
      "Created a chunk of size 1988, which is longer than the specified 1500\n",
      "Created a chunk of size 2130, which is longer than the specified 1500\n",
      "Created a chunk of size 1977, which is longer than the specified 1500\n",
      "Created a chunk of size 1585, which is longer than the specified 1500\n",
      "Created a chunk of size 1559, which is longer than the specified 1500\n",
      "Created a chunk of size 1504, which is longer than the specified 1500\n",
      "Created a chunk of size 1545, which is longer than the specified 1500\n",
      "Created a chunk of size 1512, which is longer than the specified 1500\n",
      "Created a chunk of size 1733, which is longer than the specified 1500\n",
      "Created a chunk of size 1538, which is longer than the specified 1500\n",
      "Created a chunk of size 2681, which is longer than the specified 1500\n",
      "Created a chunk of size 1976, which is longer than the specified 1500\n",
      "Created a chunk of size 2693, which is longer than the specified 1500\n",
      "Created a chunk of size 2007, which is longer than the specified 1500\n",
      "Created a chunk of size 1692, which is longer than the specified 1500\n",
      "Created a chunk of size 1773, which is longer than the specified 1500\n",
      "Created a chunk of size 3061, which is longer than the specified 1500\n",
      "Created a chunk of size 1848, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2174, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 2859, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 2212, which is longer than the specified 1500\n",
      "Created a chunk of size 2435, which is longer than the specified 1500\n",
      "Created a chunk of size 1607, which is longer than the specified 1500\n",
      "Created a chunk of size 1779, which is longer than the specified 1500\n",
      "Created a chunk of size 1955, which is longer than the specified 1500\n",
      "Created a chunk of size 1989, which is longer than the specified 1500\n",
      "Created a chunk of size 1529, which is longer than the specified 1500\n",
      "Created a chunk of size 1544, which is longer than the specified 1500\n",
      "Created a chunk of size 1523, which is longer than the specified 1500\n",
      "Created a chunk of size 1578, which is longer than the specified 1500\n",
      "Created a chunk of size 1843, which is longer than the specified 1500\n",
      "Created a chunk of size 2361, which is longer than the specified 1500\n",
      "Created a chunk of size 1665, which is longer than the specified 1500\n",
      "Created a chunk of size 1740, which is longer than the specified 1500\n",
      "Created a chunk of size 1766, which is longer than the specified 1500\n",
      "Created a chunk of size 1656, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12135 existing IDs from LanceDB.\n",
      "Created 1850 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  91%|█████████▏| 53/58 [32:11<05:51, 70.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_VII_DIVWAG.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 3dbf65da-e582-435d-a5b7-93548c30680d)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 58/58 [35:00<00:00, 36.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 1849\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 5\n",
      "Total chunked_docs   : 1850\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Volume_VII_DIVWAG.md\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 152 to 155\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_V_Part_II_DIVWAG_Prog_Manual.md document\n",
      "too many tokens, attempting to shorten document with 805097 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md document\n",
      "too many tokens, attempting to shorten document with 820016 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md document\n",
      "too many tokens, attempting to shorten document with 1486762 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_VII_Testing_Report.md document\n",
      "too many tokens, attempting to shorten document with 777204 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "error at ./final_docling_output/WAGCAP_Volume_VII_Testing_Report.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Improvement of the War-Gaming Capability (WAGCAP), Volume VII - WAGCAP Testing Report\", \"date_of_pub\": \"1972-08-01\", \"agency\": [\"US Army\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"USACDC\"], \"category\": \"capability\", \"purpose\": \"The purpose of this report is to document the testing performed on the DIVWAG model to ensure that it acceptably simulates the combat, combat support, and combat service support operations of a division force.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='USACDC', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1763, which is longer than the specified 1500\n",
      "Created a chunk of size 4797, which is longer than the specified 1500\n",
      "Created a chunk of size 1637, which is longer than the specified 1500\n",
      "Created a chunk of size 2262, which is longer than the specified 1500\n",
      "Created a chunk of size 1563, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 2079, which is longer than the specified 1500\n",
      "Created a chunk of size 1848, which is longer than the specified 1500\n",
      "Created a chunk of size 1572, which is longer than the specified 1500\n",
      "Created a chunk of size 2190, which is longer than the specified 1500\n",
      "Created a chunk of size 1600, which is longer than the specified 1500\n",
      "Created a chunk of size 2065, which is longer than the specified 1500\n",
      "Created a chunk of size 1560, which is longer than the specified 1500\n",
      "Created a chunk of size 1856, which is longer than the specified 1500\n",
      "Created a chunk of size 1676, which is longer than the specified 1500\n",
      "Created a chunk of size 2066, which is longer than the specified 1500\n",
      "Created a chunk of size 1656, which is longer than the specified 1500\n",
      "Created a chunk of size 1767, which is longer than the specified 1500\n",
      "Created a chunk of size 1532, which is longer than the specified 1500\n",
      "Created a chunk of size 1683, which is longer than the specified 1500\n",
      "Created a chunk of size 1934, which is longer than the specified 1500\n",
      "Created a chunk of size 1504, which is longer than the specified 1500\n",
      "Created a chunk of size 1665, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 1502, which is longer than the specified 1500\n",
      "Created a chunk of size 2310, which is longer than the specified 1500\n",
      "Created a chunk of size 2015, which is longer than the specified 1500\n",
      "Created a chunk of size 1744, which is longer than the specified 1500\n",
      "Created a chunk of size 1855, which is longer than the specified 1500\n",
      "Created a chunk of size 1748, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13984 existing IDs from LanceDB.\n",
      "Created 2446 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  42%|████▏     | 32/77 [19:16<29:57, 39.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 45a4b7d2-273c-4c0e-b2c8-34617d888a53)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  43%|████▎     | 33/77 [21:23<48:29, 66.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: d6e80791-a22b-4922-a479-e076d5317223)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  48%|████▊     | 37/77 [25:38<49:29, 74.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: cb996fbf-07fe-485c-86e5-e90dca1944e9)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  53%|█████▎    | 41/77 [30:09<49:44, 82.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: e75c0ae4-f2e8-44ce-9bc7-efab094c18a3)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  73%|███████▎  | 56/77 [41:35<25:33, 73.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 7e112fc2-e4c6-4972-9281-30008477990f)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  90%|████████▉ | 69/77 [52:39<08:49, 66.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_V_Part_II_DIVWAG_Prog_Manual.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 09d94d7f-cf92-414f-9b3e-6a915774f420)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 77/77 [57:06<00:00, 44.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 2440\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 11\n",
      "Total chunked_docs   : 2446\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Volume_VII_DIVWAG.md\n",
      " ...and 6 more.\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 156 to 159\n",
      "Starting on ./final_docling_output/WG_6-13_CTOC_Report.md document\n",
      "Starting on ./final_docling_output/Warriors_Edge.md document\n",
      "error at ./final_docling_output/Warriors_Edge.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Warriors Edge Simulation and Gaming System: The Squad Simulation\", \"date_of_pub\": \"2005-08-01\", \"agency\": [\"U.S. Army Research Laboratory\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"None\"], \"category\": \"capability\", \"purpose\": \"The purpose of the wargame is to develop a simulation system for a dismounted infantry squad, with the goal of providing a realistic and immersive environment for training and testing military tactics and strategies.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='None', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/Wargaming_Effectiveness_2006.md document\n",
      "time out error, pausing for 10 seconds\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md document\n",
      "too many tokens, attempting to shorten document with 1002320 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1712, which is longer than the specified 1500\n",
      "Created a chunk of size 1583, which is longer than the specified 1500\n",
      "Created a chunk of size 2151, which is longer than the specified 1500\n",
      "Created a chunk of size 1568, which is longer than the specified 1500\n",
      "Created a chunk of size 1916, which is longer than the specified 1500\n",
      "Created a chunk of size 1916, which is longer than the specified 1500\n",
      "Created a chunk of size 1916, which is longer than the specified 1500\n",
      "Created a chunk of size 1916, which is longer than the specified 1500\n",
      "Created a chunk of size 1792, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16424 existing IDs from LanceDB.\n",
      "Created 1180 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  11%|█         | 4/37 [02:04<17:33, 31.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: bcb7359b-b464-46d4-942c-a08a57572858)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  14%|█▎        | 5/37 [04:31<39:03, 73.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 89b5be9e-c2a4-4a4c-8163-ced8d0a85dae)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  68%|██████▊   | 25/37 [18:51<14:07, 70.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction failed for ./final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: bf489315-20dd-4bab-90fd-3b6d77cd730a)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 37/37 [23:51<00:00, 38.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 1177\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 14\n",
      "Total chunked_docs   : 1180\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Volume_VII_DIVWAG.md\n",
      " ...and 9 more.\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 160 to 161\n",
      "Starting on ./final_docling_output/wmd_Wargaming_1994.md document\n",
      "too many tokens, attempting to shorten document with 894102 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "Starting on ./final_docling_output/Wing_and_a_Prayer.md document\n",
      "error at ./final_docling_output/Wing_and_a_Prayer.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"A Wing and a Prayer\", \"date_of_pub\": \"2009-04-01\", \"agency\": [\"USDA-APHIS\", \"CNA\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"USDA-APHIS\"], \"category\": \"process\", \"purpose\": \"To examine the integral connections between field actions and laboratory response during an avian influenza outbreak and to identify lessons learned and best practices for the National Animal Health Laboratory Network (NAHLN).\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='USDA-APHIS', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1632, which is longer than the specified 1500\n",
      "Created a chunk of size 1566, which is longer than the specified 1500\n",
      "Created a chunk of size 1777, which is longer than the specified 1500\n",
      "Created a chunk of size 1649, which is longer than the specified 1500\n",
      "Created a chunk of size 1681, which is longer than the specified 1500\n",
      "Created a chunk of size 1569, which is longer than the specified 1500\n",
      "Created a chunk of size 1712, which is longer than the specified 1500\n",
      "Created a chunk of size 1523, which is longer than the specified 1500\n",
      "Created a chunk of size 1542, which is longer than the specified 1500\n",
      "Created a chunk of size 1555, which is longer than the specified 1500\n",
      "Created a chunk of size 1845, which is longer than the specified 1500\n",
      "Created a chunk of size 1809, which is longer than the specified 1500\n",
      "Created a chunk of size 2061, which is longer than the specified 1500\n",
      "Created a chunk of size 1965, which is longer than the specified 1500\n",
      "Created a chunk of size 1768, which is longer than the specified 1500\n",
      "Created a chunk of size 1627, which is longer than the specified 1500\n",
      "Created a chunk of size 2044, which is longer than the specified 1500\n",
      "Created a chunk of size 1511, which is longer than the specified 1500\n",
      "Created a chunk of size 1652, which is longer than the specified 1500\n",
      "Created a chunk of size 1987, which is longer than the specified 1500\n",
      "Created a chunk of size 2106, which is longer than the specified 1500\n",
      "Created a chunk of size 1668, which is longer than the specified 1500\n",
      "Created a chunk of size 1504, which is longer than the specified 1500\n",
      "Created a chunk of size 1610, which is longer than the specified 1500\n",
      "Created a chunk of size 1696, which is longer than the specified 1500\n",
      "Created a chunk of size 1566, which is longer than the specified 1500\n",
      "Created a chunk of size 1577, which is longer than the specified 1500\n",
      "Created a chunk of size 1977, which is longer than the specified 1500\n",
      "Created a chunk of size 1726, which is longer than the specified 1500\n",
      "Created a chunk of size 1610, which is longer than the specified 1500\n",
      "Created a chunk of size 1587, which is longer than the specified 1500\n",
      "Created a chunk of size 1740, which is longer than the specified 1500\n",
      "Created a chunk of size 1517, which is longer than the specified 1500\n",
      "Created a chunk of size 1517, which is longer than the specified 1500\n",
      "Created a chunk of size 1517, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17601 existing IDs from LanceDB.\n",
      "Created 772 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [11:50<00:00, 28.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 772\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 14\n",
      "Total chunked_docs   : 772\n",
      "Failed chunks:\n",
      " - ./final_docling_output/QUICK.md\n",
      " - ./final_docling_output/SCYLLA_III-73_Quick_Look.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      " - ./final_docling_output/WAGCAP_Volume_VII_DIVWAG.md\n",
      " ...and 9 more.\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs_flat=docs_flat_full #allows you to test the code without running the whole dang thing\n",
    "doc_batch_size=4\n",
    "full_doc_metadata={}\n",
    "error_log = {} \n",
    "failed_chunks = []\n",
    "# Suppress deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=PydanticDeprecatedSince20)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "for j in range(0, len(docs_flat), doc_batch_size):\n",
    "    remove_doc=[]\n",
    "    cutoff=j+doc_batch_size-1\n",
    "    if cutoff>len(docs_flat):\n",
    "        cutoff=len(docs_flat)-1\n",
    "    print(f'Working documents: {j} to {cutoff}')\n",
    "    current_docs=docs_flat[j:j+doc_batch_size]\n",
    "    full_doc_metadata={}\n",
    "    for doc in reversed(current_docs): \n",
    "        print(f'Starting on {doc.metadata['source']} document')\n",
    "        try:\n",
    "            json_output=chain.invoke({\"wargame_report\":doc.page_content})  \n",
    "            full_doc_metadata[doc.metadata['source']]=json_output.model_dump()\n",
    "        except Exception as e:\n",
    "            if \"422\" in str(e):\n",
    "                print(f\"too many tokens, attempting to shorten document with {len(doc.page_content)} characters\")\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    if is_bad_ocr(doc.page_content):\n",
    "                        print(\"detected bad ocr, cleaning and retrying API call\")\n",
    "                        text=clean_ocr_text(doc.page_content)\n",
    "                        text=doc.page_content[:150000] +\"...\"                        \n",
    "                        json_output=chain.invoke({\"wargame_report\":text})  \n",
    "                        full_doc_metadata[doc.metadata['source']]=json_output.model_dump() \n",
    "                    else:\n",
    "                        print(\"good ocr, document is just too long, trimming end\") \n",
    "                        text=clean_ocr_text(doc.page_content)\n",
    "                        text=doc.page_content[:150000] +\"...\"\n",
    "                        json_output=chain.invoke({\"wargame_report\":text})  \n",
    "                        full_doc_metadata[doc.metadata['source']]=json_output.model_dump() \n",
    "                except Exception as f:\n",
    "                    print(f'error at {doc.metadata['source']}')\n",
    "                    print(f'error code: {f}')\n",
    "                    error_log[doc.metadata['source']]= str(f)\n",
    "                    current_docs.remove(doc)\n",
    "            elif \"Read timed out.\" in str(e):\n",
    "                print(\"time out error, pausing for 10 seconds\")\n",
    "                time.sleep(10)\n",
    "                try:\n",
    "                    json_output=chain.invoke({\"wargame_report\":doc.page_content})  \n",
    "                    full_doc_metadata[doc.metadata['source']]=json_output.model_dump() \n",
    "                except Exception as f:\n",
    "                    print(\"time out error not fixed\")\n",
    "                    print(f'error at {doc.metadata['source']}')\n",
    "                    print(f'error code: {f}')\n",
    "                    error_log[doc.metadata['source']]= str(f)\n",
    "                    current_docs.remove(doc)          \n",
    "           # if str(e)\n",
    "            else:\n",
    "                print(f'error at {doc.metadata['source']}')\n",
    "                print(f'error code: {e}')\n",
    "                error_log[doc.metadata['source']]= str(e)\n",
    "                current_docs.remove(doc)\n",
    "\n",
    "    # Create IDs for each chunk so we're not inserting duplicates\n",
    "    existing_ids = set()\n",
    "    print()\n",
    "    try:\n",
    "        existing_ids = set(row[\"id\"] for row in table.to_arrow().to_pylist())\n",
    "        print(f\"Loaded {len(existing_ids)} existing IDs from LanceDB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load existing IDs. Starting fresh. Reason: {e}\")\n",
    "\n",
    "    # Initialize counters\n",
    "    skipped = 0\n",
    "    processed = 0\n",
    "\n",
    "\n",
    "    # Batch size for LLM + TEI\n",
    "    batch_size = 32\n",
    "    updates = []\n",
    "    texts_to_embed = []\n",
    "    rows_to_embed = []\n",
    "\n",
    "    #remove docs where API call failed\n",
    "\n",
    "    # Apply splitter to your loaded docs\n",
    "    chunked_docs = []\n",
    "    for doc in current_docs:\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_docs.append({\n",
    "                \"text\": chunk,\n",
    "                \"file_path\": doc.metadata.get(\"source\", \"\"),\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "\n",
    "    print(f\"Created {len(chunked_docs)} text chunks.\")\n",
    "    # Processing loop\n",
    "    for i in tqdm(range(0, len(chunked_docs), batch_size), desc=\"Processing chunks\"):\n",
    "        batch = chunked_docs[i:i + batch_size]\n",
    "\n",
    "        # Filter out already-inserted chunks\n",
    "        filtered_batch = []\n",
    "        for chunk in batch:\n",
    "            row_id = f\"{chunk['file_path']}#{chunk['chunk_index']}\"\n",
    "            if row_id not in existing_ids:\n",
    "                filtered_batch.append((row_id, chunk))\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "        if not filtered_batch:\n",
    "            continue\n",
    "\n",
    "        # Parallelize metadata extraction\n",
    "        # Create worker threads\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            # Submit tasks to the executor\n",
    "            futures = {\n",
    "                executor.submit(extract_metadata, chunk[\"text\"], chunk[\"file_path\"]): (row_id, chunk)\n",
    "                for row_id, chunk in filtered_batch\n",
    "            }\n",
    "            # Process results as they complete\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    metadata = future.result()\n",
    "                    row_id, chunk = futures[future]\n",
    "                    # Add the remaining schema-required fields\n",
    "                    # ** unpack the metadata dict\n",
    "                    row_metadata = ChunkMetadata(\n",
    "                        **{\n",
    "                            **metadata,\n",
    "                            **full_doc_metadata[chunk[\"file_path\"]]\n",
    "                        }\n",
    "                    ).model_dump()\n",
    "                    # Package into full LanceDB-compatible row\n",
    "                    rows_to_embed.append({\n",
    "                        \"id\": row_id,\n",
    "                        \"text\": chunk[\"text\"],\n",
    "                        \"metadata\": row_metadata\n",
    "                    })\n",
    "                    texts_to_embed.append(chunk[\"text\"])\n",
    "                    processed += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    file_path = futures[future][1]['file_path']\n",
    "                    print(f\"Metadata extraction failed for {file_path}: {e}\")\n",
    "                    failed_chunks.append(file_path)\n",
    "\n",
    "        # Run TEI Embedding\n",
    "        if texts_to_embed:\n",
    "            # Embed the texts in batches\n",
    "            vectors = safe_embed_documents(rows_to_embed)\n",
    "\n",
    "            # Add vectors to the rows\n",
    "            # zip the vectors with the rows of text\n",
    "            for row, vector in zip(rows_to_embed, vectors):\n",
    "                updates.append({\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"vector\": vector,\n",
    "                    \"text\": row[\"text\"],\n",
    "                    \"metadata\": row[\"metadata\"]\n",
    "                })\n",
    "\n",
    "            existing_ids.update(row[\"id\"] for row in rows_to_embed)\n",
    "            rows_to_embed = []\n",
    "            texts_to_embed = []\n",
    "\n",
    "        # Add LanceDB updates in batches\n",
    "        # Check if the updates list has reached the batch size\n",
    "        if len(updates) >= 256:\n",
    "            table.add(updates)\n",
    "            updates = []\n",
    "\n",
    "    # Only for the last batch which may not be full\n",
    "    if texts_to_embed:\n",
    "        vectors = safe_embed_documents(rows_to_embed)\n",
    "\n",
    "        for row, vector in zip(rows_to_embed, vectors):\n",
    "            updates.append({\n",
    "                \"id\": row[\"id\"],\n",
    "                \"vector\": vector,\n",
    "                \"text\": row[\"text\"],\n",
    "                \"metadata\": row[\"metadata\"]\n",
    "            })\n",
    "        existing_ids.update(row[\"id\"] for row in rows_to_embed)\n",
    "    # final catch for updates\n",
    "    if updates:\n",
    "        table.add(updates)\n",
    "\n",
    "    # Final Report\n",
    "    print(\"\\nFinished processing.\")\n",
    "    print(f\"Chunks inserted      : {processed}\")\n",
    "    print(f\"Chunks skipped       : {skipped}\")\n",
    "    print(f\"Chunks failed (LLM)  : {len(failed_chunks)}\")\n",
    "    print(f\"Total chunked_docs   : {len(chunked_docs)}\")\n",
    "\n",
    "    if failed_chunks:\n",
    "        print(\"Failed chunks:\")\n",
    "        for path in failed_chunks[:5]:\n",
    "            print(f\" - {path}\")\n",
    "        if len(failed_chunks) > 5:\n",
    "            print(f\" ...and {len(failed_chunks) - 5} more.\")\n",
    "\n",
    "    print(\"\\n \\n\")     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f18f4855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./final_docling_output/QUICK.md',\n",
       " './final_docling_output/SCYLLA_III-73_Quick_Look.md',\n",
       " './final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md',\n",
       " './final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md',\n",
       " './final_docling_output/WAGCAP_Volume_VII_DIVWAG.md',\n",
       " './final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md',\n",
       " './final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md',\n",
       " './final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md',\n",
       " './final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md',\n",
       " './final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md',\n",
       " './final_docling_output/WAGCAP_Volume_V_Part_II_DIVWAG_Prog_Manual.md',\n",
       " './final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md',\n",
       " './final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md',\n",
       " './final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac62c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All failed paths are present in the master list.\n"
     ]
    }
   ],
   "source": [
    "failed_paths = [\n",
    "    \"./final_docling_output/CAMMS_v_CPX.md\",\n",
    "    \"./final_docling_output/Counter-Insurgency_Study.md\",\n",
    "    \"./final_docling_output/Commercial_Satellite_Report.md\",\n",
    "    \"./final_docling_output/ExerciseViking22_Final_Report.md\",\n",
    "    \"./final_docling_output/Olympiad_I-62.md\",\n",
    "    \"./final_docling_output/ODNA_Transformation_Strategy_GameIV.md\",\n",
    "    \"./final_docling_output/paper_370.md\",\n",
    "    \"./final_docling_output/POLITICA_and_AGILE-COIN_1966.md\",\n",
    "    \"./final_docling_output/QUICK.md\",\n",
    "    \"./final_docling_output/WAGCAP_Volume_VII_Testing_Report.md\",\n",
    "    \"./final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md\",\n",
    "    \"./final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md\",\n",
    "    \"./final_docling_output/WAGCAP_Volume_V_Part_II_DIVWAG_Prog_Manual.md\",\n",
    "    \"./final_docling_output/SCYLLA_III-73_Quick_Look.md\",\n",
    "    \"./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\",\n",
    "    \"./final_docling_output/WAGCAP_Volume_VII_DIVWAG.md\",\n",
    "    \"./final_docling_output/Warriors_Edge.md\",\n",
    "    \"./final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md\",\n",
    "    \"./final_docling_output/Wing_and_a_Prayer.md\",\n",
    "    \"./final_docling_output/WAGCAP_Phase_II_AppC.md\",\n",
    "    \"./final_docling_output/WAGCAP_Volume_I.md\",\n",
    "    \"./final_docling_output/Stress_Testing_Financial_Crisis_2007.md\",\n",
    "    \"./final_docling_output/Theater_Battle_Model_Volume_VII.md\",\n",
    "    \"./final_docling_output/WAGCAP_Phase_II.md\"\n",
    "]\n",
    "# get list of paths to md files in specified directory\n",
    "directory = \"./final_docling_output\"\n",
    "doc_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if filename.endswith('.md')]\n",
    "\n",
    "# Compare lists\n",
    "missing_from_master = [path for path in failed_paths if path not in doc_paths]\n",
    "\n",
    "# Output results\n",
    "if missing_from_master:\n",
    "    print(\"The following failed paths are NOT in the master list:\")\n",
    "    for path in missing_from_master:\n",
    "        print(f\" - {path}\")\n",
    "else:\n",
    "    print(\"All failed paths are present in the master list.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22dc4907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [TextLoader(doc_path).load() for doc_path in failed_paths]\n",
    "docs_flat_partial = [item for sublist in docs for item in sublist]\n",
    "len(docs_flat_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96271501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working documents: 0 to 3\n",
      "Starting on ./final_docling_output/ExerciseViking22_Final_Report.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/ExerciseViking22_Final_Report.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 90214190-c011-43fb-b271-b05c88cb83e8)')\n",
      "Starting on ./final_docling_output/Commercial_Satellite_Report.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Commercial_Satellite_Report.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 5030e6a6-5ed9-46f2-a1e4-79df980b50c4)')\n",
      "Starting on ./final_docling_output/Counter-Insurgency_Study.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Counter-Insurgency_Study.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: a855aeb6-7ce4-4945-b3ae-b1e19012a934)')\n",
      "Starting on ./final_docling_output/CAMMS_v_CPX.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/CAMMS_v_CPX.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: aca2d806-ce75-4e50-9f5c-482de255f46b)')\n",
      "\n",
      "Loaded 18373 existing IDs from LanceDB.\n",
      "Created 0 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 0\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 0\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 4 to 7\n",
      "Starting on ./final_docling_output/POLITICA_and_AGILE-COIN_1966.md document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/POLITICA_and_AGILE-COIN_1966.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: a8a29e8b-04d5-4ca8-b67c-1bbeed51bedf)')\n",
      "Starting on ./final_docling_output/paper_370.md document\n",
      "Starting on ./final_docling_output/ODNA_Transformation_Strategy_GameIV.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/ODNA_Transformation_Strategy_GameIV.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: fd25ef52-2231-4634-ac52-9b283ac8ae0e)')\n",
      "Starting on ./final_docling_output/Olympiad_I-62.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Olympiad_I-62.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: 23033046-a099-40b4-a1a6-f6dc4068e977)')\n",
      "\n",
      "Loaded 18373 existing IDs from LanceDB.\n",
      "Created 51 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 2/2 [00:47<00:00, 23.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 51\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 51\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 8 to 11\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md document\n",
      "too many tokens, attempting to shorten document with 820016 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "error at ./final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Improvement of the War Gaming Capability (WAGCAP)\", \"date_of_pub\": \"1972-08-01\", \"agency\": [\"US Army Combat Developments Command\", \"Computer Sciences Corporation\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"OTHER\"], \"category\": \"process\", \"purpose\": \"The purpose of this wargame is to improve the war-gaming capability of the US Army by developing a new wargame model, known as the Division Wargame (DIVWAG), which can be used to simulate various military scenarios and test different strategies.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='OTHER', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/WAGCAP_Volume_VI_DIVWAG_Data_Req.md document\n",
      "too many tokens, attempting to shorten document with 1486762 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_VII_Testing_Report.md document\n",
      "too many tokens, attempting to shorten document with 777204 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "error at ./final_docling_output/WAGCAP_Volume_VII_Testing_Report.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Improvement of the War-Gaming Capability (WAGCAP), Volume VII - WAGCAP Testing Report\", \"date_of_pub\": \"1972-08-01\", \"agency\": [\"US Army\", \"Computer Sciences Corporation\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"USACDC\"], \"category\": \"capability\", \"purpose\": \"The purpose of this wargame is to test and evaluate the improvement of the war-gaming capability, specifically the DIVWAG model, through a series of tests and demonstrations.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='USACDC', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/QUICK.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 2155, which is longer than the specified 1500\n",
      "Created a chunk of size 1763, which is longer than the specified 1500\n",
      "Created a chunk of size 4797, which is longer than the specified 1500\n",
      "Created a chunk of size 1637, which is longer than the specified 1500\n",
      "Created a chunk of size 2262, which is longer than the specified 1500\n",
      "Created a chunk of size 1563, which is longer than the specified 1500\n",
      "Created a chunk of size 1525, which is longer than the specified 1500\n",
      "Created a chunk of size 2079, which is longer than the specified 1500\n",
      "Created a chunk of size 1848, which is longer than the specified 1500\n",
      "Created a chunk of size 1572, which is longer than the specified 1500\n",
      "Created a chunk of size 2190, which is longer than the specified 1500\n",
      "Created a chunk of size 1600, which is longer than the specified 1500\n",
      "Created a chunk of size 2065, which is longer than the specified 1500\n",
      "Created a chunk of size 1560, which is longer than the specified 1500\n",
      "Created a chunk of size 1856, which is longer than the specified 1500\n",
      "Created a chunk of size 1676, which is longer than the specified 1500\n",
      "Created a chunk of size 2066, which is longer than the specified 1500\n",
      "Created a chunk of size 1656, which is longer than the specified 1500\n",
      "Created a chunk of size 1767, which is longer than the specified 1500\n",
      "Created a chunk of size 1532, which is longer than the specified 1500\n",
      "Created a chunk of size 1683, which is longer than the specified 1500\n",
      "Created a chunk of size 1934, which is longer than the specified 1500\n",
      "Created a chunk of size 1504, which is longer than the specified 1500\n",
      "Created a chunk of size 1665, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 2422, which is longer than the specified 1500\n",
      "Created a chunk of size 1502, which is longer than the specified 1500\n",
      "Created a chunk of size 2310, which is longer than the specified 1500\n",
      "Created a chunk of size 2015, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18424 existing IDs from LanceDB.\n",
      "Created 1374 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 43/43 [00:10<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 4\n",
      "Chunks skipped       : 1370\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 1374\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 12 to 15\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_VII_DIVWAG.md document\n",
      "too many tokens, attempting to shorten document with 816095 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md document\n",
      "too many tokens, attempting to shorten document with 894488 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "error at ./final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"AD-768 163: IMPROVEMENT OF THE WAR-GAMING CAPABILITY, PHASE II\", \"date_of_pub\": \"1973-06-01\", \"agency\": [\"Combat Developments Command\", \"Computer Sciences Corporation\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"USACACD\"], \"category\": \"capability\", \"purpose\": \"The purpose of this wargame is to improve the war-gaming capability of the US military by conducting sensitivity tests on the Division War Game (DWG) model.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='USACACD', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/SCYLLA_III-73_Quick_Look.md document\n",
      "too many tokens, attempting to shorten document with 572189 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_V_Part_II_DIVWAG_Prog_Manual.md document\n",
      "too many tokens, attempting to shorten document with 805097 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1855, which is longer than the specified 1500\n",
      "Created a chunk of size 1748, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1501, which is longer than the specified 1500\n",
      "Created a chunk of size 1800, which is longer than the specified 1500\n",
      "Created a chunk of size 1847, which is longer than the specified 1500\n",
      "Created a chunk of size 1825, which is longer than the specified 1500\n",
      "Created a chunk of size 1664, which is longer than the specified 1500\n",
      "Created a chunk of size 1931, which is longer than the specified 1500\n",
      "Created a chunk of size 1813, which is longer than the specified 1500\n",
      "Created a chunk of size 2292, which is longer than the specified 1500\n",
      "Created a chunk of size 1731, which is longer than the specified 1500\n",
      "Created a chunk of size 1512, which is longer than the specified 1500\n",
      "Created a chunk of size 1850, which is longer than the specified 1500\n",
      "Created a chunk of size 3347, which is longer than the specified 1500\n",
      "Created a chunk of size 1544, which is longer than the specified 1500\n",
      "Created a chunk of size 1523, which is longer than the specified 1500\n",
      "Created a chunk of size 1578, which is longer than the specified 1500\n",
      "Created a chunk of size 1843, which is longer than the specified 1500\n",
      "Created a chunk of size 2361, which is longer than the specified 1500\n",
      "Created a chunk of size 1665, which is longer than the specified 1500\n",
      "Created a chunk of size 1740, which is longer than the specified 1500\n",
      "Created a chunk of size 1766, which is longer than the specified 1500\n",
      "Created a chunk of size 1656, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18428 existing IDs from LanceDB.\n",
      "Created 1748 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 55/55 [00:10<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 3\n",
      "Chunks skipped       : 1745\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 1748\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 16 to 19\n",
      "Starting on ./final_docling_output/WAGCAP_Phase_II_AppC.md document\n",
      "too many tokens, attempting to shorten document with 992921 characters\n",
      "detected bad ocr, cleaning and retrying API call\n",
      "error at ./final_docling_output/WAGCAP_Phase_II_AppC.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"IMPROVEMENT OF THE WAR-GAMING CAPABILITY, PHASE II\", \"date_of_pub\": \"1973-06-01\", \"agency\": [\"US Army\", \"Computer Sciences Corporation\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"USACDC\"], \"category\": \"capability\", \"purpose\": \"The purpose of this wargame is to improve the war-gaming capability of the US Army by modifying and maintaining the Division War Game (DIVWAG) model.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='USACDC', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/Wing_and_a_Prayer.md document\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_V_Part_I_DIVWAG_Prog_Manual.md document\n",
      "too many tokens, attempting to shorten document with 1002320 characters\n",
      "good ocr, document is just too long, trimming end\n",
      "Starting on ./final_docling_output/Warriors_Edge.md document\n",
      "error at ./final_docling_output/Warriors_Edge.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Warriors Edge Simulation and Gaming System: The Squad Simulation\", \"date_of_pub\": \"2005-08-01\", \"agency\": [\"U.S. Army Research Laboratory\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"OTHER\"], \"category\": \"capability\", \"purpose\": \"The purpose of the wargame is to develop and test a squad simulation system for the U.S. Army, with a focus on urban operations and the integration of web-based information technologies into military operations.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='OTHER', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1712, which is longer than the specified 1500\n",
      "Created a chunk of size 1583, which is longer than the specified 1500\n",
      "Created a chunk of size 2151, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18431 existing IDs from LanceDB.\n",
      "Created 918 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 29/29 [01:55<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 129\n",
      "Chunks skipped       : 789\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 918\n",
      "\n",
      " \n",
      "\n",
      "Working documents: 20 to 23\n",
      "Starting on ./final_docling_output/WAGCAP_Phase_II.md document\n",
      "error at ./final_docling_output/WAGCAP_Phase_II.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Improvement of the War Gaming Capability, Phase II (WAGCAP II)\", \"date_of_pub\": \"1973-06-01\", \"agency\": [\"US Army Combat Developments Command\"], \"domain\": [\"Land\"], \"country\": [\"United States\"], \"cocom\": [\"USACDC\"], \"category\": \"capability\", \"purpose\": \"The purpose of this report is to describe the work performed and results achieved by Computer Sciences Corporation's Combat Developments Research Office in response to USACDC Work Directive 6-72, Improvement of the War Gaming Capability, Phase II.\"}. Got: 1 validation error for SearchSchema\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value='USACDC', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/Theater_Battle_Model_Volume_VII.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Theater_Battle_Model_Volume_VII.md\n",
      "error code: Failed to parse SearchSchema from completion {\"title\": \"Theater Battle Model\", \"date_of_pub\": \"1968-01-01\", \"agency\": [\"Joint War Games Agency\", \"Research Analysis Corporation\"], \"domain\": [{\"$ref\": \"#/$defs/Domain\"}], \"country\": [\"United States\", \"Canada\", \"United Kingdom\"], \"cocom\": [{\"$ref\": \"#/$defs/COCOM\"}], \"category\": {\"$ref\": \"#/$defs/Wargame_type\"}, \"purpose\": \"The purpose of the TBM-68 is to revise and expand the Theater Battle Model-63 to provide a valid and economic means of evaluating planned military operations.\"}. Got: 3 validation errors for SearchSchema\n",
      "domain.0\n",
      "  Input should be 'Land', 'Maritime', 'Air', 'Space', 'Cyberspace' or 'Other' [type=enum, input_value={'$ref': '#/$defs/Domain'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "cocom.0\n",
      "  Input should be 'INDOPACOM', 'PACOM', 'CENTCOM', 'EUCOM', 'AFRICOM', 'NORTHCOM', 'SOUTHCOM', 'SPACECOM', 'CYBERCOM', 'SOCOM', 'STRATCOM', 'TRANSCOM' or 'Other' [type=enum, input_value={'$ref': '#/$defs/COCOM'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "category\n",
      "  Input should be 'policy', 'capability', 'conop', 'process' or 'other' [type=enum, input_value={'$ref': '#/$defs/Wargame_type'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/enum\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "Starting on ./final_docling_output/Stress_Testing_Financial_Crisis_2007.md document\n",
      "time out error, pausing for 10 seconds\n",
      "time out error not fixed\n",
      "error at ./final_docling_output/Stress_Testing_Financial_Crisis_2007.md\n",
      "error code: (ReadTimeoutError(\"HTTPConnectionPool(host='trac-malenia.ern.nps.edu', port=8080): Read timed out. (read timeout=120)\"), '(Request ID: d8bdcaec-a456-46c5-8740-ddf627ff89d5)')\n",
      "Starting on ./final_docling_output/WAGCAP_Volume_I.md document\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1582, which is longer than the specified 1500\n",
      "Created a chunk of size 1560, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18560 existing IDs from LanceDB.\n",
      "Created 101 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 4/4 [01:45<00:00, 26.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Chunks inserted      : 101\n",
      "Chunks skipped       : 0\n",
      "Chunks failed (LLM)  : 0\n",
      "Total chunked_docs   : 101\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Run again against failed paths\n",
    "\n",
    "docs_flat=docs_flat_partial #allows you to test the code without running the whole dang thing\n",
    "doc_batch_size=4\n",
    "full_doc_metadata={}\n",
    "error_log = {} \n",
    "failed_chunks = []\n",
    "# Suppress deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=PydanticDeprecatedSince20)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
    "for j in range(0, len(docs_flat), doc_batch_size):\n",
    "    remove_doc=[]\n",
    "    cutoff=j+doc_batch_size-1\n",
    "    if cutoff>len(docs_flat):\n",
    "        cutoff=len(docs_flat)-1\n",
    "    print(f'Working documents: {j} to {cutoff}')\n",
    "    current_docs=docs_flat[j:j+doc_batch_size]\n",
    "    full_doc_metadata={}\n",
    "    for doc in reversed(current_docs): \n",
    "        print(f'Starting on {doc.metadata['source']} document')\n",
    "        try:\n",
    "            json_output=chain.invoke({\"wargame_report\":doc.page_content})  \n",
    "            full_doc_metadata[doc.metadata['source']]=json_output.model_dump()\n",
    "        except Exception as e:\n",
    "            if \"422\" in str(e):\n",
    "                print(f\"too many tokens, attempting to shorten document with {len(doc.page_content)} characters\")\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    if is_bad_ocr(doc.page_content):\n",
    "                        print(\"detected bad ocr, cleaning and retrying API call\")\n",
    "                        text=clean_ocr_text(doc.page_content)\n",
    "                        text=doc.page_content[:150000] +\"...\"                        \n",
    "                        json_output=chain.invoke({\"wargame_report\":text})  \n",
    "                        full_doc_metadata[doc.metadata['source']]=json_output.model_dump() \n",
    "                    else:\n",
    "                        print(\"good ocr, document is just too long, trimming end\") \n",
    "                        text=clean_ocr_text(doc.page_content)\n",
    "                        text=doc.page_content[:150000] +\"...\"\n",
    "                        json_output=chain.invoke({\"wargame_report\":text})  \n",
    "                        full_doc_metadata[doc.metadata['source']]=json_output.model_dump() \n",
    "                except Exception as f:\n",
    "                    print(f'error at {doc.metadata['source']}')\n",
    "                    print(f'error code: {f}')\n",
    "                    error_log[doc.metadata['source']]= str(f)\n",
    "                    current_docs.remove(doc)\n",
    "            elif \"Read timed out.\" in str(e):\n",
    "                print(\"time out error, pausing for 10 seconds\")\n",
    "                time.sleep(10)\n",
    "                try:\n",
    "                    json_output=chain.invoke({\"wargame_report\":doc.page_content})  \n",
    "                    full_doc_metadata[doc.metadata['source']]=json_output.model_dump() \n",
    "                except Exception as f:\n",
    "                    print(\"time out error not fixed\")\n",
    "                    print(f'error at {doc.metadata['source']}')\n",
    "                    print(f'error code: {f}')\n",
    "                    error_log[doc.metadata['source']]= str(f)\n",
    "                    current_docs.remove(doc)          \n",
    "           # if str(e)\n",
    "            else:\n",
    "                print(f'error at {doc.metadata['source']}')\n",
    "                print(f'error code: {e}')\n",
    "                error_log[doc.metadata['source']]= str(e)\n",
    "                current_docs.remove(doc)\n",
    "\n",
    "    # Create IDs for each chunk so we're not inserting duplicates\n",
    "    existing_ids = set()\n",
    "    print()\n",
    "    try:\n",
    "        existing_ids = set(row[\"id\"] for row in table.to_arrow().to_pylist())\n",
    "        print(f\"Loaded {len(existing_ids)} existing IDs from LanceDB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load existing IDs. Starting fresh. Reason: {e}\")\n",
    "\n",
    "    # Initialize counters\n",
    "    skipped = 0\n",
    "    processed = 0\n",
    "\n",
    "\n",
    "    # Batch size for LLM + TEI\n",
    "    batch_size = 32\n",
    "    updates = []\n",
    "    texts_to_embed = []\n",
    "    rows_to_embed = []\n",
    "\n",
    "    #remove docs where API call failed\n",
    "\n",
    "    # Apply splitter to your loaded docs\n",
    "    chunked_docs = []\n",
    "    for doc in current_docs:\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_docs.append({\n",
    "                \"text\": chunk,\n",
    "                \"file_path\": doc.metadata.get(\"source\", \"\"),\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "\n",
    "    print(f\"Created {len(chunked_docs)} text chunks.\")\n",
    "    # Processing loop\n",
    "    for i in tqdm(range(0, len(chunked_docs), batch_size), desc=\"Processing chunks\"):\n",
    "        batch = chunked_docs[i:i + batch_size]\n",
    "\n",
    "        # Filter out already-inserted chunks\n",
    "        filtered_batch = []\n",
    "        for chunk in batch:\n",
    "            row_id = f\"{chunk['file_path']}#{chunk['chunk_index']}\"\n",
    "            if row_id not in existing_ids:\n",
    "                filtered_batch.append((row_id, chunk))\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "        if not filtered_batch:\n",
    "            continue\n",
    "\n",
    "        # Parallelize metadata extraction\n",
    "        # Create worker threads\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            # Submit tasks to the executor\n",
    "            futures = {\n",
    "                executor.submit(extract_metadata, chunk[\"text\"], chunk[\"file_path\"]): (row_id, chunk)\n",
    "                for row_id, chunk in filtered_batch\n",
    "            }\n",
    "            # Process results as they complete\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    metadata = future.result()\n",
    "                    row_id, chunk = futures[future]\n",
    "                    # Add the remaining schema-required fields\n",
    "                    # ** unpack the metadata dict\n",
    "                    row_metadata = ChunkMetadata(\n",
    "                        **{\n",
    "                            **metadata,\n",
    "                            **full_doc_metadata[chunk[\"file_path\"]]\n",
    "                        }\n",
    "                    ).model_dump()\n",
    "                    # Package into full LanceDB-compatible row\n",
    "                    rows_to_embed.append({\n",
    "                        \"id\": row_id,\n",
    "                        \"text\": chunk[\"text\"],\n",
    "                        \"metadata\": row_metadata\n",
    "                    })\n",
    "                    texts_to_embed.append(chunk[\"text\"])\n",
    "                    processed += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    file_path = futures[future][1]['file_path']\n",
    "                    print(f\"Metadata extraction failed for {file_path}: {e}\")\n",
    "                    failed_chunks.append(file_path)\n",
    "\n",
    "        # Run TEI Embedding\n",
    "        if texts_to_embed:\n",
    "            # Embed the texts in batches\n",
    "            vectors = safe_embed_documents(rows_to_embed)\n",
    "\n",
    "            # Add vectors to the rows\n",
    "            # zip the vectors with the rows of text\n",
    "            for row, vector in zip(rows_to_embed, vectors):\n",
    "                updates.append({\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"vector\": vector,\n",
    "                    \"text\": row[\"text\"],\n",
    "                    \"metadata\": row[\"metadata\"]\n",
    "                })\n",
    "\n",
    "            existing_ids.update(row[\"id\"] for row in rows_to_embed)\n",
    "            rows_to_embed = []\n",
    "            texts_to_embed = []\n",
    "\n",
    "        # Add LanceDB updates in batches\n",
    "        # Check if the updates list has reached the batch size\n",
    "        if len(updates) >= 256:\n",
    "            table.add(updates)\n",
    "            updates = []\n",
    "\n",
    "    # Only for the last batch which may not be full\n",
    "    if texts_to_embed:\n",
    "        vectors = safe_embed_documents(rows_to_embed)\n",
    "\n",
    "        for row, vector in zip(rows_to_embed, vectors):\n",
    "            updates.append({\n",
    "                \"id\": row[\"id\"],\n",
    "                \"vector\": vector,\n",
    "                \"text\": row[\"text\"],\n",
    "                \"metadata\": row[\"metadata\"]\n",
    "            })\n",
    "        existing_ids.update(row[\"id\"] for row in rows_to_embed)\n",
    "    # final catch for updates\n",
    "    if updates:\n",
    "        table.add(updates)\n",
    "\n",
    "    # Final Report\n",
    "    print(\"\\nFinished processing.\")\n",
    "    print(f\"Chunks inserted      : {processed}\")\n",
    "    print(f\"Chunks skipped       : {skipped}\")\n",
    "    print(f\"Chunks failed (LLM)  : {len(failed_chunks)}\")\n",
    "    print(f\"Total chunked_docs   : {len(chunked_docs)}\")\n",
    "\n",
    "    if failed_chunks:\n",
    "        print(\"Failed chunks:\")\n",
    "        for path in failed_chunks[:5]:\n",
    "            print(f\" - {path}\")\n",
    "        if len(failed_chunks) > 5:\n",
    "            print(f\" ...and {len(failed_chunks) - 5} more.\")\n",
    "\n",
    "    print(\"\\n \\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fd4666d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./final_docling_output/ExerciseViking22_Final_Report.md',\n",
       " './final_docling_output/Commercial_Satellite_Report.md',\n",
       " './final_docling_output/Counter-Insurgency_Study.md',\n",
       " './final_docling_output/CAMMS_v_CPX.md',\n",
       " './final_docling_output/POLITICA_and_AGILE-COIN_1966.md',\n",
       " './final_docling_output/ODNA_Transformation_Strategy_GameIV.md',\n",
       " './final_docling_output/Olympiad_I-62.md',\n",
       " './final_docling_output/WAGCAP_Volume_V_Part_III_DIVWAG_Prog_Manual.md',\n",
       " './final_docling_output/WAGCAP_Volume_VII_Testing_Report.md',\n",
       " './final_docling_output/WAGCAP_Phase_II_AppB_Part_II.md',\n",
       " './final_docling_output/WAGCAP_Phase_II_AppC.md',\n",
       " './final_docling_output/Warriors_Edge.md',\n",
       " './final_docling_output/WAGCAP_Phase_II.md',\n",
       " './final_docling_output/Theater_Battle_Model_Volume_VII.md',\n",
       " './final_docling_output/Stress_Testing_Financial_Crisis_2007.md']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(error_log.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oa4910",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
